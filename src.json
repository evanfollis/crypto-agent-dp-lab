{
  "tests/test_data_ingest.py": "\"\"\"\nTests for crypto data ingestion module.\n\"\"\"\n\nimport pytest\nimport tempfile\nimport os\nfrom unittest.mock import Mock, patch\nimport polars as pl\nimport jax.numpy as jnp\n\nfrom crypto_dp.data.ingest import (\n    fetch_ohlcv,\n    fetch_coingecko_data,\n    load_to_duck,\n    get_top_crypto_symbols,\n    create_sample_dataset\n)\n\n\nclass TestFetchOHLCV:\n    \"\"\"Test OHLCV data fetching functionality.\"\"\"\n    \n    @patch('crypto_dp.data.ingest.ccxt')\n    def test_fetch_ohlcv_success(self, mock_ccxt):\n        \"\"\"Test successful OHLCV data fetching.\"\"\"\n        # Mock exchange\n        mock_exchange = Mock()\n        mock_exchange.fetch_ohlcv.return_value = [\n            [1640995200000, 47000.0, 48000.0, 46000.0, 47500.0, 1.5],\n            [1640998800000, 47500.0, 48500.0, 47000.0, 48000.0, 2.0],\n        ]\n        mock_ccxt.binance.return_value = mock_exchange\n        \n        # Test fetch\n        start_time = 1640995200000\n        end_time = 1641081600000\n        \n        df = fetch_ohlcv(\"BTC/USDT\", start_time, end_time, \"1h\", \"binance\")\n        \n        assert isinstance(df, pl.DataFrame)\n        assert len(df) == 2\n        assert \"symbol\" in df.columns\n        assert \"timestamp\" in df.columns\n        assert \"open\" in df.columns\n        assert df[\"symbol\"][0] == \"BTC/USDT\"\n    \n    @patch('crypto_dp.data.ingest.ccxt')\n    def test_fetch_ohlcv_empty_data(self, mock_ccxt):\n        \"\"\"Test handling of empty OHLCV data.\"\"\"\n        mock_exchange = Mock()\n        mock_exchange.fetch_ohlcv.return_value = []\n        mock_ccxt.binance.return_value = mock_exchange\n        \n        df = fetch_ohlcv(\"BTC/USDT\", 0, 1000, \"1h\")\n        \n        assert isinstance(df, pl.DataFrame)\n        assert len(df) == 0\n    \n    @patch('crypto_dp.data.ingest.ccxt')\n    def test_fetch_ohlcv_error_handling(self, mock_ccxt):\n        \"\"\"Test error handling in OHLCV fetching.\"\"\"\n        mock_exchange = Mock()\n        mock_exchange.fetch_ohlcv.side_effect = Exception(\"API Error\")\n        mock_ccxt.binance.return_value = mock_exchange\n        \n        with pytest.raises(Exception, match=\"API Error\"):\n            fetch_ohlcv(\"BTC/USDT\", 0, 1000, \"1h\")\n\n\nclass TestFetchCoinGeckoData:\n    \"\"\"Test CoinGecko data fetching functionality.\"\"\"\n    \n    @patch('crypto_dp.data.ingest.CoinGeckoAPI')\n    def test_fetch_coingecko_data_success(self, mock_cg_class):\n        \"\"\"Test successful CoinGecko data fetching.\"\"\"\n        mock_cg = Mock()\n        mock_cg.get_coin_market_chart_by_id.return_value = {\n            'prices': [[1640995200000, 47000.0], [1640998800000, 47500.0]],\n            'market_caps': [[1640995200000, 900000000000], [1640998800000, 910000000000]],\n            'total_volumes': [[1640995200000, 30000000000], [1640998800000, 32000000000]]\n        }\n        mock_cg_class.return_value = mock_cg\n        \n        df = fetch_coingecko_data([\"bitcoin\"], \"usd\", 7)\n        \n        assert isinstance(df, pl.DataFrame)\n        assert len(df) == 2\n        assert \"coin_id\" in df.columns\n        assert \"price\" in df.columns\n        assert \"market_cap\" in df.columns\n        assert df[\"coin_id\"][0] == \"bitcoin\"\n    \n    @patch('crypto_dp.data.ingest.CoinGeckoAPI')\n    def test_fetch_coingecko_data_error(self, mock_cg_class):\n        \"\"\"Test error handling in CoinGecko fetching.\"\"\"\n        mock_cg = Mock()\n        mock_cg.get_coin_market_chart_by_id.side_effect = Exception(\"Rate limit\")\n        mock_cg_class.return_value = mock_cg\n        \n        df = fetch_coingecko_data([\"bitcoin\"], \"usd\", 7)\n        \n        # Should return empty DataFrame on error\n        assert isinstance(df, pl.DataFrame)\n        assert len(df) == 0\n\n\nclass TestDuckDBOperations:\n    \"\"\"Test DuckDB loading and querying operations.\"\"\"\n    \n    def test_load_to_duck_replace(self):\n        \"\"\"Test loading data to DuckDB with replace mode.\"\"\"\n        with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as tmp_file:\n            db_path = tmp_file.name\n        # Remove the empty file so DuckDB can create a new database\n        os.unlink(db_path)\n        \n        try:\n            # Create test data\n            df = pl.DataFrame({\n                \"timestamp\": [1640995200000, 1640998800000],\n                \"symbol\": [\"BTC/USDT\", \"BTC/USDT\"],\n                \"price\": [47000.0, 47500.0],\n                \"volume\": [1.5, 2.0]\n            })\n            \n            # Load to DuckDB\n            load_to_duck(db_path, df, \"test_table\", \"replace\")\n            \n            # Verify data was loaded\n            import duckdb\n            con = duckdb.connect(db_path)\n            result = con.execute(\"SELECT COUNT(*) FROM test_table\").fetchone()\n            assert result[0] == 2\n            con.close()\n            \n        finally:\n            if os.path.exists(db_path):\n                os.unlink(db_path)\n\n\nclass TestHelperFunctions:\n    \"\"\"Test utility and helper functions.\"\"\"\n    \n    @patch('crypto_dp.data.ingest.CoinGeckoAPI')\n    def test_get_top_crypto_symbols(self, mock_cg_class):\n        \"\"\"Test getting top crypto symbols.\"\"\"\n        mock_cg = Mock()\n        mock_cg.get_coins_markets.return_value = [\n            {\"symbol\": \"btc\", \"market_cap\": 900000000000},\n            {\"symbol\": \"eth\", \"market_cap\": 400000000000},\n            {\"symbol\": \"bnb\", \"market_cap\": 80000000000},\n        ]\n        mock_cg_class.return_value = mock_cg\n        \n        symbols = get_top_crypto_symbols(3)\n        \n        assert len(symbols) <= 3\n        assert all(\"/\" in symbol for symbol in symbols)  # Should be trading pairs\n        assert \"BTC/USDT\" in symbols\n        assert \"ETH/USDT\" in symbols\n    \n    @patch('crypto_dp.data.ingest.CoinGeckoAPI')\n    def test_get_top_crypto_symbols_error(self, mock_cg_class):\n        \"\"\"Test error handling in getting top symbols.\"\"\"\n        mock_cg = Mock()\n        mock_cg.get_coins_markets.side_effect = Exception(\"API Error\")\n        mock_cg_class.return_value = mock_cg\n        \n        symbols = get_top_crypto_symbols(5)\n        \n        assert symbols == []  # Should return empty list on error\n\n\nclass TestCreateSampleDataset:\n    \"\"\"Test sample dataset creation functionality.\"\"\"\n    \n    @pytest.mark.slow\n    @pytest.mark.network\n    @patch('crypto_dp.data.ingest.get_top_crypto_symbols')\n    @patch('crypto_dp.data.ingest.fetch_ohlcv')\n    def test_create_sample_dataset(self, mock_fetch, mock_symbols):\n        \"\"\"Test creating a sample dataset.\"\"\"\n        # Mock dependencies\n        mock_symbols.return_value = [\"BTC/USDT\", \"ETH/USDT\"]\n        mock_fetch.return_value = pl.DataFrame({\n            \"timestamp\": [1640995200000],\n            \"symbol\": [\"BTC/USDT\"],\n            \"open\": [47000.0],\n            \"high\": [48000.0],\n            \"low\": [46000.0],\n            \"close\": [47500.0],\n            \"volume\": [1.5],\n            \"timeframe\": [\"1h\"],\n            \"exchange\": [\"binance\"],\n            \"datetime\": [\"2022-01-01 00:00:00\"]\n        })\n        \n        with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as tmp_file:\n            db_path = tmp_file.name\n        \n        try:\n            # Create sample dataset\n            create_sample_dataset(db_path, None, 1)  # 1 day of data\n            \n            # Should not raise exceptions\n            assert os.path.exists(db_path)\n            \n        finally:\n            if os.path.exists(db_path):\n                os.unlink(db_path)\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])",
  "tests/test_portfolio.py": "\"\"\"\nTests for differentiable portfolio optimization module.\n\"\"\"\n\nimport pytest\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\n\nfrom crypto_dp.models.portfolio import (\n    softmax_weights,\n    gumbel_softmax_weights,\n    long_only_weights,\n    long_short_weights,\n    sharpe_ratio,\n    information_ratio,\n    max_drawdown,\n    transaction_cost_penalty,\n    concentration_penalty,\n    DifferentiablePortfolio,\n    portfolio_objective,\n    portfolio_step,\n    backtest_portfolio\n)\n\n\nclass TestWeightTransformations:\n    \"\"\"Test various weight transformation functions.\"\"\"\n    \n    def test_softmax_weights(self):\n        \"\"\"Test softmax weight transformation.\"\"\"\n        scores = jnp.array([1.0, 2.0, 3.0])\n        weights = softmax_weights(scores, temperature=1.0)\n        \n        assert weights.shape == scores.shape\n        assert jnp.allclose(jnp.sum(weights), 1.0)\n        assert jnp.all(weights >= 0.0)\n        \n        # Higher scores should get higher weights\n        assert weights[2] > weights[1] > weights[0]\n    \n    def test_softmax_weights_temperature(self):\n        \"\"\"Test temperature effect in softmax.\"\"\"\n        scores = jnp.array([1.0, 2.0, 3.0])\n        \n        # High temperature should make weights more uniform\n        weights_high_temp = softmax_weights(scores, temperature=10.0)\n        weights_low_temp = softmax_weights(scores, temperature=0.1)\n        \n        # Low temperature should be more concentrated (higher max weight)\n        assert jnp.max(weights_low_temp) > jnp.max(weights_high_temp)\n    \n    def test_gumbel_softmax_weights(self):\n        \"\"\"Test Gumbel-softmax weight transformation.\"\"\"\n        scores = jnp.array([1.0, 2.0, 3.0])\n        key = jax.random.PRNGKey(42)\n        \n        weights = gumbel_softmax_weights(scores, temperature=1.0, key=key)\n        \n        assert weights.shape == scores.shape\n        assert jnp.allclose(jnp.sum(weights), 1.0, atol=1e-6)\n        assert jnp.all(weights >= 0.0)\n    \n    def test_long_only_weights(self):\n        \"\"\"Test long-only weight transformation.\"\"\"\n        scores = jnp.array([-1.0, 2.0, 3.0])\n        weights = long_only_weights(scores)\n        \n        assert weights.shape == scores.shape\n        assert jnp.allclose(jnp.sum(weights), 1.0)\n        assert jnp.all(weights >= 0.0)\n        \n        # Negative scores should result in zero weights\n        assert weights[0] == 0.0\n        assert weights[1] > 0.0\n        assert weights[2] > 0.0\n    \n    def test_long_only_weights_all_negative(self):\n        \"\"\"Test long-only weights with all negative scores.\"\"\"\n        scores = jnp.array([-1.0, -2.0, -3.0])\n        weights = long_only_weights(scores)\n        \n        # Should fall back to equal weights\n        expected_weight = 1.0 / len(scores)\n        assert jnp.allclose(weights, expected_weight)\n    \n    def test_long_short_weights(self):\n        \"\"\"Test long-short weight transformation.\"\"\"\n        scores = jnp.array([-2.0, 1.0, 3.0])\n        weights = long_short_weights(scores, long_weight=1.0, short_weight=1.0)\n        \n        assert weights.shape == scores.shape\n        \n        # Should have both positive and negative weights\n        assert jnp.any(weights > 0.0)\n        assert jnp.any(weights < 0.0)\n        \n        # Positive scores should give positive weights\n        assert weights[1] > 0.0\n        assert weights[2] > 0.0\n        \n        # Negative scores should give negative weights\n        assert weights[0] < 0.0\n\n\nclass TestRiskMetrics:\n    \"\"\"Test risk and performance metrics.\"\"\"\n    \n    def test_sharpe_ratio(self):\n        \"\"\"Test Sharpe ratio calculation.\"\"\"\n        # Create synthetic returns with positive expected return\n        returns = jnp.array([\n            [0.01, 0.02, -0.01],\n            [0.02, -0.01, 0.01],\n            [0.03, 0.01, 0.02],\n            [-0.01, 0.03, 0.01]\n        ])\n        weights = jnp.array([0.4, 0.4, 0.2])\n        \n        sharpe = sharpe_ratio(returns, weights)\n        \n        assert jnp.isscalar(sharpe)\n        assert jnp.isfinite(sharpe)\n        # Should be negative (since we return negative Sharpe for minimization)\n        assert sharpe <= 0.0\n    \n    def test_information_ratio(self):\n        \"\"\"Test information ratio calculation.\"\"\"\n        returns = jnp.array([\n            [0.01, 0.02],\n            [0.02, -0.01],\n            [0.03, 0.01],\n            [-0.01, 0.03]\n        ])\n        weights = jnp.array([0.6, 0.4])\n        benchmark_returns = jnp.array([0.015, 0.005, 0.02, 0.01])\n        \n        ir = information_ratio(returns, weights, benchmark_returns)\n        \n        assert jnp.isscalar(ir)\n        assert jnp.isfinite(ir)\n    \n    def test_max_drawdown(self):\n        \"\"\"Test maximum drawdown calculation.\"\"\"\n        # Create returns that lead to a drawdown\n        returns = jnp.array([\n            [0.10, 0.05],   # Positive returns\n            [0.05, 0.02],   # More positive\n            [-0.15, -0.10], # Large negative (drawdown)\n            [-0.05, -0.03], # More negative\n            [0.08, 0.06]    # Recovery\n        ])\n        weights = jnp.array([0.6, 0.4])\n        \n        dd = max_drawdown(returns, weights)\n        \n        assert jnp.isscalar(dd)\n        assert jnp.isfinite(dd)\n        assert dd <= 0.0  # Drawdown should be negative\n    \n    def test_transaction_cost_penalty(self):\n        \"\"\"Test transaction cost calculation.\"\"\"\n        old_weights = jnp.array([0.3, 0.4, 0.3])\n        new_weights = jnp.array([0.4, 0.3, 0.3])\n        \n        cost = transaction_cost_penalty(old_weights, new_weights, cost_rate=0.001)\n        \n        assert jnp.isscalar(cost)\n        assert jnp.isfinite(cost)\n        assert cost >= 0.0\n        \n        # No change should result in zero cost\n        zero_cost = transaction_cost_penalty(old_weights, old_weights)\n        assert zero_cost == 0.0\n    \n    def test_concentration_penalty(self):\n        \"\"\"Test concentration penalty calculation.\"\"\"\n        # Highly concentrated weights\n        concentrated_weights = jnp.array([0.8, 0.1, 0.1])\n        penalty = concentration_penalty(concentrated_weights, max_weight=0.2)\n        \n        assert jnp.isscalar(penalty)\n        assert jnp.isfinite(penalty)\n        assert penalty > 0.0  # Should penalize concentration\n        \n        # Diversified weights\n        diversified_weights = jnp.array([0.15, 0.15, 0.15, 0.15, 0.4])\n        low_penalty = concentration_penalty(diversified_weights, max_weight=0.5)\n        \n        assert low_penalty < penalty\n\n\nclass TestDifferentiablePortfolio:\n    \"\"\"Test differentiable portfolio model.\"\"\"\n    \n    def test_portfolio_initialization(self):\n        \"\"\"Test portfolio model initialization.\"\"\"\n        input_dim = 20\n        n_assets = 5\n        key = jax.random.PRNGKey(42)\n        \n        model = DifferentiablePortfolio(\n            input_dim=input_dim,\n            n_assets=n_assets,\n            key=key\n        )\n        \n        assert isinstance(model.scoring_network, eqx.nn.MLP)\n        assert callable(model.weight_transform)\n    \n    def test_portfolio_forward_single(self):\n        \"\"\"Test portfolio forward pass with single sample.\"\"\"\n        input_dim = 10\n        n_assets = 3\n        key = jax.random.PRNGKey(42)\n        \n        model = DifferentiablePortfolio(\n            input_dim=input_dim,\n            n_assets=n_assets,\n            key=key\n        )\n        \n        features = jax.random.normal(key, (input_dim,))\n        weights = model(features)\n        \n        assert weights.shape == (n_assets,)\n        assert jnp.allclose(jnp.sum(weights), 1.0, atol=1e-6)\n        assert jnp.all(weights >= 0.0)  # Default is softmax (long-only)\n    \n    def test_portfolio_forward_batch(self):\n        \"\"\"Test portfolio forward pass with batch.\"\"\"\n        input_dim = 10\n        n_assets = 3\n        batch_size = 5\n        key = jax.random.PRNGKey(42)\n        \n        model = DifferentiablePortfolio(\n            input_dim=input_dim,\n            n_assets=n_assets,\n            key=key\n        )\n        \n        features = jax.random.normal(key, (batch_size, input_dim))\n        weights = model(features)\n        \n        assert weights.shape == (batch_size, n_assets)\n        assert jnp.allclose(jnp.sum(weights, axis=1), 1.0, atol=1e-6)\n    \n    def test_portfolio_different_transforms(self):\n        \"\"\"Test different weight transformation methods.\"\"\"\n        input_dim = 5\n        n_assets = 3\n        key = jax.random.PRNGKey(42)\n        features = jax.random.normal(key, (input_dim,))\n        \n        # Test different weight transformations\n        transforms = [\"softmax\", \"long_only\", \"long_short\"]\n        \n        for transform in transforms:\n            model = DifferentiablePortfolio(\n                input_dim=input_dim,\n                n_assets=n_assets,\n                weight_transform=transform,\n                key=key\n            )\n            \n            weights = model(features)\n            assert weights.shape == (n_assets,)\n            assert jnp.isfinite(weights).all()\n\n\nclass TestPortfolioOptimization:\n    \"\"\"Test portfolio optimization functions.\"\"\"\n    \n    def test_portfolio_objective(self):\n        \"\"\"Test portfolio objective function.\"\"\"\n        input_dim = 5\n        n_assets = 3\n        n_periods = 20\n        key = jax.random.PRNGKey(42)\n        \n        model = DifferentiablePortfolio(\n            input_dim=input_dim,\n            n_assets=n_assets,\n            key=key\n        )\n        \n        features = jax.random.normal(key, (input_dim,))\n        returns = jax.random.normal(jax.random.split(key)[0], (n_periods, n_assets)) * 0.01\n        \n        objective = portfolio_objective(model, features, returns)\n        \n        assert jnp.isscalar(objective)\n        assert jnp.isfinite(objective)\n    \n    def test_portfolio_step(self):\n        \"\"\"Test single portfolio optimization step.\"\"\"\n        input_dim = 5\n        n_assets = 3\n        n_periods = 20\n        key = jax.random.PRNGKey(42)\n        \n        model = DifferentiablePortfolio(\n            input_dim=input_dim,\n            n_assets=n_assets,\n            key=key\n        )\n        \n        features = jax.random.normal(key, (input_dim,))\n        returns = jax.random.normal(jax.random.split(key)[0], (n_periods, n_assets)) * 0.01\n        \n        updated_model, loss, weights = portfolio_step(\n            model, features, returns, learning_rate=1e-3\n        )\n        \n        assert isinstance(updated_model, DifferentiablePortfolio)\n        assert jnp.isscalar(loss)\n        assert jnp.isfinite(loss)\n        assert weights.shape == (n_assets,)\n        \n        # Model should be updated\n        original_weights = model(features)\n        assert not jnp.allclose(weights, original_weights, atol=1e-6)\n    \n    def test_portfolio_step_with_transaction_costs(self):\n        \"\"\"Test optimization step with transaction costs.\"\"\"\n        input_dim = 5\n        n_assets = 3\n        n_periods = 20\n        key = jax.random.PRNGKey(42)\n        \n        model = DifferentiablePortfolio(\n            input_dim=input_dim,\n            n_assets=n_assets,\n            key=key\n        )\n        \n        features = jax.random.normal(key, (input_dim,))\n        returns = jax.random.normal(jax.random.split(key)[0], (n_periods, n_assets)) * 0.01\n        old_weights = jnp.array([0.3, 0.4, 0.3])\n        \n        updated_model, loss, weights = portfolio_step(\n            model, features, returns, old_weights=old_weights, beta=0.1\n        )\n        \n        assert isinstance(updated_model, DifferentiablePortfolio)\n        assert jnp.isscalar(loss)\n        assert jnp.isfinite(loss)\n\n\nclass TestBacktesting:\n    \"\"\"Test portfolio backtesting functionality.\"\"\"\n    \n    def test_backtest_portfolio(self):\n        \"\"\"Test portfolio backtesting.\"\"\"\n        input_dim = 5\n        n_assets = 3\n        n_periods = 300  # Need enough periods for lookback\n        key = jax.random.PRNGKey(42)\n        \n        model = DifferentiablePortfolio(\n            input_dim=input_dim,\n            n_assets=n_assets,\n            key=key\n        )\n        \n        features_sequence = jax.random.normal(key, (n_periods, input_dim))\n        returns_sequence = jax.random.normal(\n            jax.random.split(key)[0], (n_periods, n_assets)\n        ) * 0.01\n        \n        portfolio_returns, weights_history, transaction_costs = backtest_portfolio(\n            model, features_sequence, returns_sequence,\n            lookback_window=252, rebalance_freq=5\n        )\n        \n        assert len(portfolio_returns) > 0\n        assert len(weights_history) > 0\n        assert len(transaction_costs) > 0\n        \n        # Check shapes\n        assert jnp.isfinite(jnp.array(portfolio_returns)).all()\n        assert weights_history.shape[1] == n_assets\n        assert jnp.isfinite(jnp.array(transaction_costs)).all()\n\n\nclass TestEdgeCases:\n    \"\"\"Test edge cases and error conditions.\"\"\"\n    \n    def test_zero_returns(self):\n        \"\"\"Test handling of zero returns.\"\"\"\n        returns = jnp.zeros((10, 3))\n        weights = jnp.array([0.3, 0.4, 0.3])\n        \n        # Should handle zero returns gracefully\n        sharpe = sharpe_ratio(returns, weights)\n        assert jnp.isfinite(sharpe)\n    \n    def test_single_asset(self):\n        \"\"\"Test portfolio with single asset.\"\"\"\n        input_dim = 5\n        n_assets = 1\n        key = jax.random.PRNGKey(42)\n        \n        model = DifferentiablePortfolio(\n            input_dim=input_dim,\n            n_assets=n_assets,\n            key=key\n        )\n        \n        features = jax.random.normal(key, (input_dim,))\n        weights = model(features)\n        \n        assert weights.shape == (1,)\n        assert jnp.allclose(jnp.sum(weights), 1.0)\n    \n    def test_invalid_weight_transform(self):\n        \"\"\"Test invalid weight transformation.\"\"\"\n        with pytest.raises(ValueError):\n            DifferentiablePortfolio(\n                input_dim=5,\n                n_assets=3,\n                weight_transform=\"invalid\"\n            )\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])",
  "tests/test_graph_scaffold.py": "\"\"\"\nTests for latent graph scaffold module.\n\"\"\"\n\nimport pytest\nimport jax\nimport jax.numpy as jnp\nimport networkx as nx\n\nfrom crypto_dp.graph.scaffold import (\n    LatentGraph,\n    bic_loss,\n    spectral_regularization,\n    graph_step,\n    train_graph,\n    create_crypto_factor_graph\n)\n\n\nclass TestLatentGraph:\n    \"\"\"Test LatentGraph functionality.\"\"\"\n    \n    def test_latent_graph_initialization(self):\n        \"\"\"Test LatentGraph initialization.\"\"\"\n        n_factors = 5\n        key = jax.random.PRNGKey(42)\n        \n        graph = LatentGraph(n_factors, key=key)\n        \n        assert graph.n_factors == n_factors\n        assert graph.W.shape == (n_factors, n_factors)\n        assert callable(graph.activation)\n    \n    def test_latent_graph_forward_single(self):\n        \"\"\"Test forward pass with single sample.\"\"\"\n        n_factors = 3\n        key = jax.random.PRNGKey(42)\n        \n        graph = LatentGraph(n_factors, key=key)\n        x = jnp.array([1.0, 2.0, 3.0])\n        \n        output = graph(x)\n        \n        assert output.shape == (n_factors,)\n        assert jnp.isfinite(output).all()\n    \n    def test_latent_graph_forward_batch(self):\n        \"\"\"Test forward pass with batch of samples.\"\"\"\n        n_factors = 4\n        batch_size = 10\n        key = jax.random.PRNGKey(42)\n        \n        graph = LatentGraph(n_factors, key=key)\n        x = jax.random.normal(key, (batch_size, n_factors))\n        \n        output = graph(x)\n        \n        assert output.shape == (batch_size, n_factors)\n        assert jnp.isfinite(output).all()\n    \n    def test_multi_step_forward(self):\n        \"\"\"Test multi-step message passing.\"\"\"\n        n_factors = 3\n        key = jax.random.PRNGKey(42)\n        \n        graph = LatentGraph(n_factors, key=key)\n        x = jnp.array([1.0, 2.0, 3.0])\n        \n        output = graph.forward_multi_step(x, n_steps=3)\n        \n        assert output.shape == (n_factors,)\n        assert jnp.isfinite(output).all()\n    \n    def test_get_adjacency_matrix(self):\n        \"\"\"Test getting adjacency matrix.\"\"\"\n        n_factors = 3\n        key = jax.random.PRNGKey(42)\n        \n        graph = LatentGraph(n_factors, key=key)\n        adj_matrix = graph.get_adjacency_matrix()\n        \n        assert adj_matrix.shape == (n_factors, n_factors)\n        assert jnp.array_equal(adj_matrix, graph.W)\n    \n    def test_get_graph_structure(self):\n        \"\"\"Test converting to NetworkX graph.\"\"\"\n        n_factors = 3\n        key = jax.random.PRNGKey(42)\n        \n        graph = LatentGraph(n_factors, key=key)\n        nx_graph = graph.get_graph_structure(threshold=0.01)\n        \n        assert isinstance(nx_graph, nx.DiGraph)\n        assert nx_graph.number_of_nodes() == n_factors\n\n\nclass TestLossFunctions:\n    \"\"\"Test loss functions for graph training.\"\"\"\n    \n    def test_bic_loss(self):\n        \"\"\"Test BIC loss computation.\"\"\"\n        n_factors = 3\n        n_samples = 50\n        key = jax.random.PRNGKey(42)\n        \n        model = LatentGraph(n_factors, key=key)\n        x = jax.random.normal(key, (n_samples, n_factors))\n        target = jax.random.normal(jax.random.split(key)[0], (n_samples, n_factors))\n        \n        loss = bic_loss(model, x, target, lambda_reg=1e-2)\n        \n        assert jnp.isscalar(loss)\n        assert jnp.isfinite(loss)\n        assert loss >= 0.0\n    \n    def test_spectral_regularization(self):\n        \"\"\"Test spectral regularization.\"\"\"\n        n_factors = 3\n        key = jax.random.PRNGKey(42)\n        \n        model = LatentGraph(n_factors, key=key)\n        reg = spectral_regularization(model, alpha=1e-3)\n        \n        assert jnp.isscalar(reg)\n        assert jnp.isfinite(reg)\n        assert reg >= 0.0\n    \n    def test_bic_loss_gradient(self):\n        \"\"\"Test that BIC loss is differentiable.\"\"\"\n        n_factors = 3\n        n_samples = 20\n        key = jax.random.PRNGKey(42)\n        \n        model = LatentGraph(n_factors, key=key)\n        x = jax.random.normal(key, (n_samples, n_factors))\n        target = jax.random.normal(jax.random.split(key)[0], (n_samples, n_factors))\n        \n        # Compute gradient\n        loss_fn = lambda m: bic_loss(m, x, target)\n        grad_fn = jax.grad(loss_fn)\n        \n        try:\n            grads = grad_fn(model)\n            # Should not raise an error\n            assert hasattr(grads, 'W')\n            assert grads.W.shape == model.W.shape\n        except Exception as e:\n            pytest.fail(f\"Gradient computation failed: {e}\")\n\n\nclass TestTraining:\n    \"\"\"Test graph training functionality.\"\"\"\n    \n    def test_graph_step(self):\n        \"\"\"Test single training step.\"\"\"\n        n_factors = 3\n        n_samples = 20\n        key = jax.random.PRNGKey(42)\n        \n        model = LatentGraph(n_factors, key=key)\n        x = jax.random.normal(key, (n_samples, n_factors))\n        target = jax.random.normal(jax.random.split(key)[0], (n_samples, n_factors))\n        \n        updated_model, loss = graph_step(model, x, target, learning_rate=1e-3)\n        \n        assert isinstance(updated_model, LatentGraph)\n        assert jnp.isscalar(loss)\n        assert jnp.isfinite(loss)\n        \n        # Model should be updated (weights changed)\n        assert not jnp.allclose(model.W, updated_model.W, atol=1e-6)\n    \n    def test_train_graph_basic(self):\n        \"\"\"Test basic graph training.\"\"\"\n        n_factors = 3\n        n_samples = 50\n        key = jax.random.PRNGKey(42)\n        \n        model = LatentGraph(n_factors, key=key)\n        x_train = jax.random.normal(key, (n_samples, n_factors))\n        y_train = jax.random.normal(jax.random.split(key)[0], (n_samples, n_factors))\n        \n        trained_model, history = train_graph(\n            model, x_train, y_train,\n            n_epochs=10,\n            learning_rate=1e-3,\n            verbose=False\n        )\n        \n        assert isinstance(trained_model, LatentGraph)\n        assert isinstance(history, dict)\n        assert 'train_loss' in history\n        assert 'spectral_radius' in history\n        assert len(history['train_loss']) == 10\n    \n    def test_train_graph_with_validation(self):\n        \"\"\"Test graph training with validation data.\"\"\"\n        n_factors = 3\n        n_samples = 30\n        key = jax.random.PRNGKey(42)\n        \n        model = LatentGraph(n_factors, key=key)\n        x_train = jax.random.normal(key, (n_samples, n_factors))\n        y_train = jax.random.normal(jax.random.split(key)[0], (n_samples, n_factors))\n        x_val = jax.random.normal(jax.random.split(key)[1], (10, n_factors))\n        y_val = jax.random.normal(jax.random.split(key)[2], (10, n_factors))\n        \n        trained_model, history = train_graph(\n            model, x_train, y_train, x_val, y_val,\n            n_epochs=5,\n            learning_rate=1e-3,\n            patience=3,\n            verbose=False\n        )\n        \n        assert isinstance(trained_model, LatentGraph)\n        assert 'val_loss' in history\n        assert len(history['val_loss']) <= 5  # May stop early\n    \n    def test_training_convergence(self):\n        \"\"\"Test that training reduces loss.\"\"\"\n        n_factors = 3\n        n_samples = 100\n        key = jax.random.PRNGKey(42)\n        \n        # Create synthetic data with some structure\n        true_W = jax.random.normal(key, (n_factors, n_factors)) * 0.1\n        x_data = jax.random.normal(jax.random.split(key)[0], (n_samples, n_factors))\n        y_data = jnp.tanh(x_data @ true_W)  # Target with structure\n        \n        model = LatentGraph(n_factors, key=key)\n        \n        trained_model, history = train_graph(\n            model, x_data, y_data,\n            n_epochs=50,\n            learning_rate=1e-2,\n            verbose=False\n        )\n        \n        # Loss should generally decrease\n        initial_loss = history['train_loss'][0]\n        final_loss = history['train_loss'][-1]\n        assert final_loss < initial_loss\n\n\nclass TestCryptoFactorGraph:\n    \"\"\"Test crypto-specific graph functionality.\"\"\"\n    \n    def test_create_crypto_factor_graph(self):\n        \"\"\"Test creating crypto factor graph.\"\"\"\n        n_assets = 10\n        n_market_factors = 5\n        key = jax.random.PRNGKey(42)\n        \n        graph = create_crypto_factor_graph(n_assets, n_market_factors, key)\n        \n        assert isinstance(graph, LatentGraph)\n        assert graph.n_factors == n_assets + n_market_factors\n        assert graph.W.shape == (n_assets + n_market_factors, n_assets + n_market_factors)\n    \n    def test_crypto_graph_forward_pass(self):\n        \"\"\"Test forward pass with crypto factor graph.\"\"\"\n        n_assets = 5\n        n_market_factors = 3\n        key = jax.random.PRNGKey(42)\n        \n        graph = create_crypto_factor_graph(n_assets, n_market_factors, key)\n        \n        # Simulate crypto features (asset prices + market factors)\n        crypto_features = jax.random.normal(key, (n_assets + n_market_factors,))\n        \n        output = graph(crypto_features)\n        \n        assert output.shape == (n_assets + n_market_factors,)\n        assert jnp.isfinite(output).all()\n\n\nclass TestEdgeCases:\n    \"\"\"Test edge cases and error conditions.\"\"\"\n    \n    def test_zero_factors(self):\n        \"\"\"Test handling of zero factors.\"\"\"\n        with pytest.raises((ValueError, IndexError)):\n            LatentGraph(0)\n    \n    def test_single_factor(self):\n        \"\"\"Test single factor graph.\"\"\"\n        key = jax.random.PRNGKey(42)\n        graph = LatentGraph(1, key=key)\n        \n        x = jnp.array([1.0])\n        output = graph(x)\n        \n        assert output.shape == (1,)\n        assert jnp.isfinite(output).all()\n    \n    def test_large_graph(self):\n        \"\"\"Test large graph handling.\"\"\"\n        n_factors = 100\n        key = jax.random.PRNGKey(42)\n        \n        graph = LatentGraph(n_factors, key=key)\n        x = jax.random.normal(key, (n_factors,))\n        \n        output = graph(x)\n        \n        assert output.shape == (n_factors,)\n        assert jnp.isfinite(output).all()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])",
  "tests/__init__.py": "# Test package for crypto_dp",
  "tests/test_rl_agent.py": "\"\"\"\nTests for RL trading agent module.\n\"\"\"\n\nimport pytest\nimport jax\nimport jax.numpy as jnp\nimport time\nfrom unittest.mock import patch\n\nfrom crypto_dp.rl.agent import (\n    TradingAction,\n    MarketState,\n    RiskLevel,\n    TradingEnvironment,\n    CryptoTradingAgent,\n    RiskManager,\n    train_agent\n)\n\n\nclass TestDataStructures:\n    \"\"\"Test data structures and enums.\"\"\"\n    \n    def test_trading_action(self):\n        \"\"\"Test TradingAction creation.\"\"\"\n        positions = jnp.array([0.1, -0.2, 0.3])\n        action = TradingAction(\n            positions=positions,\n            confidence=0.8,\n            timestamp=time.time()\n        )\n        \n        assert jnp.array_equal(action.positions, positions)\n        assert action.confidence == 0.8\n        assert isinstance(action.timestamp, float)\n    \n    def test_market_state(self):\n        \"\"\"Test MarketState creation.\"\"\"\n        state = MarketState(\n            prices=jnp.array([100.0, 200.0, 50.0]),\n            features=jnp.array([1.0, 2.0, 3.0]),\n            portfolio=jnp.array([0.3, 0.4, 0.3]),\n            cash=10000.0,\n            timestamp=time.time()\n        )\n        \n        assert state.prices.shape == (3,)\n        assert state.features.shape == (3,)\n        assert state.portfolio.shape == (3,)\n        assert state.cash == 10000.0\n    \n    def test_risk_level(self):\n        \"\"\"Test RiskLevel enum.\"\"\"\n        assert RiskLevel.LOW.value == 0.5\n        assert RiskLevel.MEDIUM.value == 1.0\n        assert RiskLevel.HIGH.value == 1.5\n        assert RiskLevel.CRITICAL.value == 2.0\n\n\nclass TestTradingEnvironment:\n    \"\"\"Test trading environment functionality.\"\"\"\n    \n    def test_environment_initialization(self):\n        \"\"\"Test trading environment initialization.\"\"\"\n        symbols = ['BTC/USDT', 'ETH/USDT', 'BNB/USDT']\n        env = TradingEnvironment(symbols, initial_cash=50000.0)\n        \n        assert env.symbols == symbols\n        assert env.n_assets == 3\n        assert env.initial_cash == 50000.0\n        assert env.cash == 50000.0\n    \n    def test_environment_reset(self):\n        \"\"\"Test environment reset.\"\"\"\n        symbols = ['BTC/USDT', 'ETH/USDT']\n        env = TradingEnvironment(symbols)\n        \n        # Modify environment state\n        env.cash = 50000.0\n        env.step_count = 10\n        \n        # Reset\n        initial_state = env.reset()\n        \n        assert env.cash == env.initial_cash\n        assert env.step_count == 0\n        assert isinstance(initial_state, MarketState)\n        assert initial_state.prices.shape == (2,)\n    \n    def test_environment_step(self):\n        \"\"\"Test environment step function.\"\"\"\n        symbols = ['BTC/USDT', 'ETH/USDT']\n        env = TradingEnvironment(symbols)\n        env.reset()\n        \n        action = TradingAction(\n            positions=jnp.array([0.1, -0.1]),\n            confidence=0.7,\n            timestamp=time.time()\n        )\n        \n        new_state, reward, done, info = env.step(action)\n        \n        assert isinstance(new_state, MarketState)\n        assert isinstance(reward, float)\n        assert isinstance(done, bool)\n        assert isinstance(info, dict)\n        \n        # Check info dictionary\n        assert 'transaction_cost' in info\n        assert 'slippage' in info\n        assert 'portfolio_value' in info\n        assert 'step_count' in info\n    \n    def test_environment_termination(self):\n        \"\"\"Test environment termination conditions.\"\"\"\n        symbols = ['BTC/USDT']\n        env = TradingEnvironment(symbols, max_position_size=0.1)\n        env.reset()\n        \n        # Create action that violates position limit\n        large_action = TradingAction(\n            positions=jnp.array([0.5]),  # Exceeds max_position_size\n            confidence=0.5,\n            timestamp=time.time()\n        )\n        \n        _, _, done, _ = env.step(large_action)\n        assert done  # Should terminate due to position limit violation\n\n\nclass TestCryptoTradingAgent:\n    \"\"\"Test crypto trading agent functionality.\"\"\"\n    \n    def test_agent_initialization(self):\n        \"\"\"Test agent initialization.\"\"\"\n        state_dim = 10\n        action_dim = 3\n        key = jax.random.PRNGKey(42)\n        \n        agent = CryptoTradingAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            key=key\n        )\n        \n        assert hasattr(agent, 'policy_network')\n        assert hasattr(agent, 'value_network')\n        assert hasattr(agent, 'risk_module')\n    \n    def test_agent_get_action_deterministic(self):\n        \"\"\"Test deterministic action generation.\"\"\"\n        state_dim = 5\n        action_dim = 3\n        key = jax.random.PRNGKey(42)\n        \n        agent = CryptoTradingAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            key=key\n        )\n        \n        state = jax.random.normal(key, (state_dim,))\n        action = agent.get_action(state, deterministic=True)\n        \n        assert isinstance(action, TradingAction)\n        assert action.positions.shape == (action_dim,)\n        assert jnp.isfinite(action.positions).all()\n        assert isinstance(action.confidence, float)\n        \n        # Deterministic actions should be repeatable\n        action2 = agent.get_action(state, deterministic=True)\n        assert jnp.allclose(action.positions, action2.positions)\n    \n    def test_agent_get_action_stochastic(self):\n        \"\"\"Test stochastic action generation.\"\"\"\n        state_dim = 5\n        action_dim = 3\n        key = jax.random.PRNGKey(42)\n        \n        agent = CryptoTradingAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            key=key\n        )\n        \n        state = jax.random.normal(key, (state_dim,))\n        action1 = agent.get_action(state, deterministic=False, key=jax.random.PRNGKey(1))\n        action2 = agent.get_action(state, deterministic=False, key=jax.random.PRNGKey(2))\n        \n        # Stochastic actions should be different\n        assert not jnp.allclose(action1.positions, action2.positions, atol=1e-6)\n    \n    def test_agent_get_value(self):\n        \"\"\"Test state value estimation.\"\"\"\n        state_dim = 5\n        action_dim = 3\n        key = jax.random.PRNGKey(42)\n        \n        agent = CryptoTradingAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            key=key\n        )\n        \n        state = jax.random.normal(key, (state_dim,))\n        value = agent.get_value(state)\n        \n        assert isinstance(value, float)\n        assert jnp.isfinite(value)\n    \n    def test_agent_market_neutral_constraint(self):\n        \"\"\"Test that agent generates approximately market-neutral positions.\"\"\"\n        state_dim = 5\n        action_dim = 4\n        key = jax.random.PRNGKey(42)\n        \n        agent = CryptoTradingAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            key=key\n        )\n        \n        state = jax.random.normal(key, (state_dim,))\n        action = agent.get_action(state, deterministic=True)\n        \n        # Positions should sum to approximately zero (market neutral)\n        assert jnp.abs(jnp.sum(action.positions)) < 0.1\n\n\nclass TestRiskManager:\n    \"\"\"Test risk management functionality.\"\"\"\n    \n    def test_risk_manager_initialization(self):\n        \"\"\"Test risk manager initialization.\"\"\"\n        rm = RiskManager(\n            max_drawdown=0.05,\n            max_daily_loss=0.02,\n            max_position_size=0.15\n        )\n        \n        assert rm.max_drawdown == 0.05\n        assert rm.max_daily_loss == 0.02\n        assert rm.max_position_size == 0.15\n    \n    def test_risk_check_safe_action(self):\n        \"\"\"Test risk check with safe action.\"\"\"\n        rm = RiskManager()\n        \n        action = TradingAction(\n            positions=jnp.array([0.1, -0.05, 0.08]),\n            confidence=0.8,\n            timestamp=time.time()\n        )\n        \n        is_safe, warnings, modified_action = rm.check_risk_limits(\n            action, portfolio_value=105000.0, volatility=0.5\n        )\n        \n        assert is_safe\n        assert len(warnings) == 0\n        assert jnp.allclose(modified_action.positions, action.positions)\n    \n    def test_risk_check_position_size_violation(self):\n        \"\"\"Test risk check with position size violation.\"\"\"\n        rm = RiskManager(max_position_size=0.1)\n        \n        action = TradingAction(\n            positions=jnp.array([0.3, -0.2, 0.1]),  # First position too large\n            confidence=0.8,\n            timestamp=time.time()\n        )\n        \n        is_safe, warnings, modified_action = rm.check_risk_limits(\n            action, portfolio_value=100000.0, volatility=0.5\n        )\n        \n        assert not is_safe\n        assert len(warnings) > 0\n        assert jnp.max(jnp.abs(modified_action.positions)) <= rm.max_position_size\n    \n    def test_risk_check_drawdown_violation(self):\n        \"\"\"Test risk check with drawdown violation.\"\"\"\n        rm = RiskManager(max_drawdown=0.05)\n        rm.peak_portfolio_value = 100000.0\n        \n        action = TradingAction(\n            positions=jnp.array([0.1, -0.05, 0.08]),\n            confidence=0.8,\n            timestamp=time.time()\n        )\n        \n        # Portfolio value below drawdown limit\n        is_safe, warnings, modified_action = rm.check_risk_limits(\n            action, portfolio_value=90000.0, volatility=0.5  # 10% drawdown\n        )\n        \n        assert not is_safe\n        assert any(\"drawdown\" in warning.lower() for warning in warnings)\n        # Should force flat positions\n        assert jnp.allclose(modified_action.positions, 0.0)\n    \n    def test_risk_check_leverage_violation(self):\n        \"\"\"Test risk check with leverage violation.\"\"\"\n        rm = RiskManager(max_leverage=1.5)\n        \n        action = TradingAction(\n            positions=jnp.array([0.8, -0.9, 0.6]),  # Total exposure = 2.3x\n            confidence=0.8,\n            timestamp=time.time()\n        )\n        \n        is_safe, warnings, modified_action = rm.check_risk_limits(\n            action, portfolio_value=100000.0, volatility=0.5\n        )\n        \n        assert not is_safe\n        assert any(\"leverage\" in warning.lower() for warning in warnings)\n        # Total exposure should be scaled down\n        total_exposure = jnp.sum(jnp.abs(modified_action.positions))\n        assert total_exposure <= rm.max_leverage + 1e-6\n    \n    def test_risk_check_high_volatility(self):\n        \"\"\"Test risk check with high volatility.\"\"\"\n        rm = RiskManager(volatility_threshold=1.0)\n        \n        action = TradingAction(\n            positions=jnp.array([0.1, -0.05, 0.08]),\n            confidence=0.8,\n            timestamp=time.time()\n        )\n        \n        is_safe, warnings, modified_action = rm.check_risk_limits(\n            action, portfolio_value=100000.0, volatility=2.0  # High volatility\n        )\n        \n        assert not is_safe\n        assert any(\"volatility\" in warning.lower() for warning in warnings)\n        # Positions should be scaled down\n        assert jnp.all(jnp.abs(modified_action.positions) < jnp.abs(action.positions))\n\n\nclass TestTraining:\n    \"\"\"Test agent training functionality.\"\"\"\n    \n    @patch('crypto_dp.rl.agent.logger')\n    def test_train_agent_basic(self, mock_logger):\n        \"\"\"Test basic agent training.\"\"\"\n        symbols = ['BTC/USDT', 'ETH/USDT']\n        env = TradingEnvironment(symbols)\n        \n        state_dim = len(symbols) + 20 + len(symbols) + 1\n        action_dim = len(symbols)\n        \n        agent = CryptoTradingAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            key=jax.random.PRNGKey(42)\n        )\n        \n        # Short training run for testing\n        trained_agent, history = train_agent(\n            agent, env,\n            n_episodes=5,\n            learning_rate=1e-3,\n            verbose=False\n        )\n        \n        assert isinstance(trained_agent, CryptoTradingAgent)\n        assert isinstance(history, dict)\n        assert 'episode_rewards' in history\n        assert 'episode_lengths' in history\n        assert 'policy_losses' in history\n        assert len(history['episode_rewards']) == 5\n    \n    def test_train_agent_history_structure(self):\n        \"\"\"Test training history structure.\"\"\"\n        symbols = ['BTC/USDT']\n        env = TradingEnvironment(symbols)\n        \n        state_dim = len(symbols) + 20 + len(symbols) + 1\n        action_dim = len(symbols)\n        \n        agent = CryptoTradingAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            key=jax.random.PRNGKey(42)\n        )\n        \n        trained_agent, history = train_agent(\n            agent, env,\n            n_episodes=3,\n            verbose=False\n        )\n        \n        # Check history structure\n        required_keys = ['episode_rewards', 'episode_lengths', 'policy_losses']\n        for key in required_keys:\n            assert key in history\n            assert len(history[key]) == 3\n            assert all(isinstance(x, (int, float)) for x in history[key])\n\n\nclass TestIntegration:\n    \"\"\"Test integration between components.\"\"\"\n    \n    def test_agent_environment_interaction(self):\n        \"\"\"Test agent interacting with environment.\"\"\"\n        symbols = ['BTC/USDT', 'ETH/USDT']\n        env = TradingEnvironment(symbols)\n        \n        state_dim = len(symbols) + 20 + len(symbols) + 1\n        action_dim = len(symbols)\n        \n        agent = CryptoTradingAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            key=jax.random.PRNGKey(42)\n        )\n        \n        # Reset environment and get initial state\n        state = env.reset()\n        \n        # Convert state to array for agent\n        state_array = jnp.concatenate([\n            state.prices,\n            state.features,\n            state.portfolio,\n            jnp.array([state.cash])\n        ])\n        \n        # Agent generates action\n        action = agent.get_action(state_array, deterministic=True)\n        \n        # Environment processes action\n        new_state, reward, done, info = env.step(action)\n        \n        assert isinstance(new_state, MarketState)\n        assert isinstance(reward, float)\n        assert isinstance(done, bool)\n        assert isinstance(info, dict)\n    \n    def test_agent_risk_manager_integration(self):\n        \"\"\"Test agent with risk manager.\"\"\"\n        state_dim = 5\n        action_dim = 3\n        key = jax.random.PRNGKey(42)\n        \n        agent = CryptoTradingAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            key=key\n        )\n        \n        risk_manager = RiskManager(max_position_size=0.1)\n        \n        state = jax.random.normal(key, (state_dim,))\n        raw_action = agent.get_action(state, deterministic=True)\n        \n        # Apply risk management\n        is_safe, warnings, safe_action = risk_manager.check_risk_limits(\n            raw_action, portfolio_value=100000.0, volatility=0.5\n        )\n        \n        # Risk manager should modify action if needed\n        assert isinstance(safe_action, TradingAction)\n        assert jnp.max(jnp.abs(safe_action.positions)) <= risk_manager.max_position_size\n\n\nclass TestEdgeCases:\n    \"\"\"Test edge cases and error conditions.\"\"\"\n    \n    def test_empty_environment(self):\n        \"\"\"Test environment with no symbols.\"\"\"\n        with pytest.raises((ValueError, IndexError)):\n            TradingEnvironment([])\n    \n    def test_agent_zero_dimensions(self):\n        \"\"\"Test agent with zero dimensions.\"\"\"\n        with pytest.raises((ValueError, IndexError)):\n            CryptoTradingAgent(state_dim=0, action_dim=0)\n    \n    def test_extreme_risk_parameters(self):\n        \"\"\"Test risk manager with extreme parameters.\"\"\"\n        # Very restrictive risk manager\n        rm = RiskManager(\n            max_drawdown=0.001,\n            max_position_size=0.001,\n            max_leverage=0.1\n        )\n        \n        action = TradingAction(\n            positions=jnp.array([0.1, -0.1]),\n            confidence=0.5,\n            timestamp=time.time()\n        )\n        \n        is_safe, warnings, modified_action = rm.check_risk_limits(\n            action, portfolio_value=100000.0, volatility=0.5\n        )\n        \n        # Should heavily restrict positions\n        assert not is_safe\n        assert len(warnings) > 0\n        assert jnp.max(jnp.abs(modified_action.positions)) <= rm.max_position_size\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])",
  "tests/test_integration_real.py": "\"\"\"\nIntegration test for real data flow - exercises the entire E2E pipeline.\n\nThis test validates that:\n1. Real data can be fetched from exchanges\n2. Data can be stored and retrieved from DuckDB\n3. Differentiable portfolio model can be trained on real data\n4. Backtest can be executed end-to-end\n\nMarked with @pytest.mark.integration for CI separation.\n\"\"\"\n\nimport pytest\nimport tempfile\nimport os\nimport time\nimport sys\nfrom pathlib import Path\n\n# Add project root to path\nsys.path.insert(0, str(Path(__file__).parent.parent.parent))\n\nimport polars as pl\nimport duckdb\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom src.crypto_dp.data.ingest import fetch_ohlcv, load_to_duck\nfrom src.crypto_dp.models.portfolio import DifferentiablePortfolio, backtest_portfolio\n\n\n@pytest.mark.integration\n@pytest.mark.network\ndef test_real_data_flow():\n    \"\"\"Test complete data flow from exchange to trained model.\"\"\"\n    \n    # Configuration for lightweight test\n    symbol = \"BTC/USDT\"\n    end_ms = int(time.time() * 1000)\n    start_ms = end_ms - 7 * 24 * 60 * 60 * 1000  # 7 days\n    \n    # Step 1: Fetch real data from exchange\n    try:\n        df = fetch_ohlcv(symbol, start_ms, end_ms, \"1h\", \"binance\")\n    except Exception as e:\n        pytest.skip(f\"Exchange API unavailable: {e}\")\n    \n    assert not df.is_empty(), \"No data returned from exchange\"\n    assert len(df) > 0, \"Empty dataframe returned\"\n    assert \"close\" in df.columns, \"Close price column missing\"\n    assert \"timestamp\" in df.columns, \"Timestamp column missing\"\n    \n    # Step 2: Store and retrieve from DuckDB\n    with tempfile.NamedTemporaryFile(suffix=\".db\", delete=False) as f:\n        db_path = f.name\n    \n    try:\n        # Store data\n        load_to_duck(db_path, df, \"ohlcv\", \"replace\")\n        \n        # Verify storage\n        con = duckdb.connect(db_path)\n        rows = con.execute(\"SELECT COUNT(*) FROM ohlcv\").fetchone()[0]\n        con.close()\n        \n        assert rows == len(df), f\"Row count mismatch: {rows} != {len(df)}\"\n        \n        # Step 3: Process data for model training\n        prices = df[\"close\"].to_numpy()\n        returns = np.diff(prices) / prices[:-1]\n        features = returns.reshape(-1, 1)  # Single asset\n        \n        assert len(features) > 10, \"Insufficient data for training\"\n        assert not np.any(np.isnan(features)), \"NaN values in features\"\n        \n        # Step 4: Create and test differentiable portfolio model\n        jax.config.update('jax_enable_x64', False)  # Use 32-bit for speed\n        key = jax.random.PRNGKey(42)\n        \n        model = DifferentiablePortfolio(\n            input_dim=1,\n            n_assets=1,\n            key=key\n        )\n        \n        # Verify model can process features\n        test_weights = model.scoring_network(features[0])\n        assert test_weights.shape == (1,), f\"Wrong weight shape: {test_weights.shape}\"\n        assert jnp.isfinite(test_weights).all(), \"Non-finite weights generated\"\n        \n        # Step 5: Run mini backtest\n        if len(features) >= 24:  # Need at least 24 hours of data\n            try:\n                port_returns, weight_history, transaction_costs = backtest_portfolio(\n                    model,\n                    features,\n                    returns.reshape(-1, 1),\n                    lookback_window=min(24, len(features) // 2),\n                    rebalance_freq=6\n                )\n                \n                assert len(port_returns) > 0, \"No portfolio returns generated\"\n                assert jnp.isfinite(port_returns).all(), \"Non-finite portfolio returns\"\n                assert len(weight_history) > 0, \"No weight history generated\"\n                \n                # Calculate basic performance metrics\n                cumulative_return = np.prod(1 + port_returns)\n                assert cumulative_return > 0, \"Invalid cumulative return\"\n                \n            except Exception as e:\n                pytest.fail(f\"Backtest failed: {e}\")\n        \n    finally:\n        # Cleanup\n        if os.path.exists(db_path):\n            os.unlink(db_path)\n\n\n@pytest.mark.integration\ndef test_minimal_training_loop():\n    \"\"\"Test that a minimal training loop works with synthetic data.\"\"\"\n    \n    # Create synthetic data that mimics real market data\n    np.random.seed(42)\n    n_timesteps = 100\n    n_assets = 2\n    \n    # Generate correlated price returns\n    returns = np.random.multivariate_normal(\n        mean=[0.0001, 0.0001],  # Small positive drift\n        cov=[[0.0004, 0.0002], [0.0002, 0.0004]],  # Realistic volatility\n        size=n_timesteps\n    )\n    features = returns.copy()\n    \n    # Initialize model\n    jax.config.update('jax_enable_x64', False)\n    key = jax.random.PRNGKey(42)\n    \n    model = DifferentiablePortfolio(\n        input_dim=n_assets,\n        n_assets=n_assets,\n        key=key\n    )\n    \n    # Test that we can compute a portfolio step\n    from src.crypto_dp.models.portfolio import portfolio_step\n    \n    try:\n        updated_model, loss, diagnostics = portfolio_step(\n            model,\n            features[0],\n            returns[:10],  # Small lookback\n            learning_rate=1e-3\n        )\n        \n        assert jnp.isfinite(loss), f\"Non-finite loss: {loss}\"\n        assert hasattr(updated_model, 'scoring_network'), \"Model structure corrupted\"\n        \n        # Test that model parameters actually changed\n        original_weights = model.scoring_network.weight\n        updated_weights = updated_model.scoring_network.weight\n        \n        weight_change = jnp.linalg.norm(updated_weights - original_weights)\n        assert weight_change > 1e-8, f\"Parameters didn't update: {weight_change}\"\n        \n    except Exception as e:\n        pytest.fail(f\"Portfolio step failed: {e}\")\n\n\n@pytest.mark.integration \n@pytest.mark.slow\ndef test_full_experiment_pipeline():\n    \"\"\"Test the full experiment pipeline with minimal data.\"\"\"\n    \n    # This test simulates the complete experiment with smaller scope\n    # to ensure the pipeline works end-to-end\n    \n    try:\n        # Import the experiment module\n        sys.path.insert(0, str(Path(__file__).parent.parent.parent / \"experiments\" / \"first_e2e\"))\n        \n        # Create minimal synthetic dataset\n        n_timesteps = 72  # 3 days of hourly data\n        n_assets = 2\n        \n        # Generate synthetic OHLCV data\n        np.random.seed(42)\n        timestamps = [int(time.time() * 1000) - i * 3600000 for i in range(n_timesteps)]\n        timestamps.reverse()\n        \n        data = []\n        symbols = ['BTC/USDT', 'ETH/USDT']\n        \n        for i, symbol in enumerate(symbols):\n            base_price = 50000 if i == 0 else 3000\n            prices = base_price * np.exp(np.cumsum(np.random.normal(0, 0.01, n_timesteps)))\n            \n            for t, (timestamp, price) in enumerate(zip(timestamps, prices)):\n                data.append({\n                    'timestamp': timestamp,\n                    'symbol': symbol,\n                    'timeframe': '1h',\n                    'exchange': 'binance',\n                    'open': price * 0.999,\n                    'high': price * 1.001,\n                    'low': price * 0.998,\n                    'close': price,\n                    'volume': np.random.uniform(100, 1000),\n                    'datetime': timestamp // 1000\n                })\n        \n        df = pl.DataFrame(data)\n        \n        # Create temporary database\n        with tempfile.NamedTemporaryFile(suffix=\".db\", delete=False) as f:\n            db_path = f.name\n        \n        try:\n            # Store data\n            load_to_duck(db_path, df, \"ohlcv\", \"replace\")\n            \n            # Process data (minimal feature pipeline)\n            con = duckdb.connect(db_path)\n            prices_df = con.execute(\"\"\"\n                SELECT datetime, symbol, close\n                FROM ohlcv\n                WHERE timeframe = '1h'\n                ORDER BY datetime\n            \"\"\").df()\n            con.close()\n            \n            pivot_df = (pl.from_pandas(prices_df)\n                       .pivot(index='datetime', columns='symbol', values='close')\n                       .sort('datetime'))\n            \n            price_matrix = pivot_df.select(pl.exclude('datetime')).to_numpy()\n            returns = np.diff(price_matrix, axis=0) / price_matrix[:-1]\n            features = returns.copy()\n            \n            # Mini training loop\n            jax.config.update('jax_enable_x64', False)\n            key = jax.random.PRNGKey(42)\n            \n            model = DifferentiablePortfolio(\n                input_dim=n_assets,\n                n_assets=n_assets,\n                key=key\n            )\n            \n            # Train for just a few steps\n            from src.crypto_dp.models.portfolio import portfolio_step\n            \n            losses = []\n            for epoch in range(10):\n                t_idx = epoch % (len(features) - 1)\n                lookback_returns = returns[max(0, t_idx-10):t_idx+1]\n                \n                model, loss, _ = portfolio_step(\n                    model,\n                    features[t_idx],\n                    lookback_returns,\n                    learning_rate=1e-3\n                )\n                losses.append(float(loss))\n            \n            # Verify training worked\n            assert len(losses) == 10, \"Training loop incomplete\"\n            assert all(jnp.isfinite(l) for l in losses), \"Non-finite losses during training\"\n            \n            # Mini backtest\n            port_returns, _, _ = backtest_portfolio(\n                model,\n                features,\n                returns,\n                lookback_window=10,\n                rebalance_freq=6\n            )\n            \n            assert len(port_returns) > 0, \"Backtest produced no returns\"\n            assert jnp.isfinite(port_returns).all(), \"Non-finite backtest returns\"\n            \n        finally:\n            if os.path.exists(db_path):\n                os.unlink(db_path)\n                \n    except Exception as e:\n        pytest.fail(f\"Full pipeline test failed: {e}\")\n\n\nif __name__ == \"__main__\":\n    # Allow running tests directly\n    pytest.main([__file__, \"-v\", \"-s\"])",
  "crypto_dp/__init__.py": "# Crypto Differentiable Programming Lab\n__version__ = \"0.0.1\"",
  "crypto_dp/monitoring/gradient_health.py": "\"\"\"\nAdvanced gradient health monitoring infrastructure for E2E-DP.\n\nBased on CLAUDE.md specifications for comprehensive gradient diagnostics\nincluding variance of absolute gradients, signal-to-total-variance ratio,\nand layer-wise analysis.\n\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import tree_util\nfrom typing import Any, Dict, List, Tuple, Optional\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport time\n\n\n@dataclass\nclass GradientMetrics:\n    \"\"\"Container for gradient health metrics.\"\"\"\n    norm_ratio: float\n    signal_to_total_variance: float\n    variance_abs_gradients: float\n    gradient_sparsity: float\n    variance_trend: Optional[float] = None\n    layer_norms: Dict[str, float] = field(default_factory=dict)\n    timestamp: float = field(default_factory=time.time)\n    \n    def is_healthy(self) -> Tuple[bool, List[str]]:\n        \"\"\"Check if metrics indicate healthy gradients.\"\"\"\n        issues = []\n        \n        # Check norm ratio (should be in [0.1, 10])\n        if self.norm_ratio < 0.1:\n            issues.append(f\"Vanishing gradient flow (norm ratio: {self.norm_ratio:.3f})\")\n        elif self.norm_ratio > 10:\n            issues.append(f\"Exploding gradient flow (norm ratio: {self.norm_ratio:.3f})\")\n        \n        # Check signal-to-total-variance\n        if self.signal_to_total_variance < 0.01:\n            issues.append(f\"Poor gradient signal (STV: {self.signal_to_total_variance:.3f})\")\n        \n        # Check sparsity\n        if self.gradient_sparsity > 0.9:\n            issues.append(f\"Excessive gradient sparsity ({self.gradient_sparsity:.1%})\")\n        \n        # Check variance trend if available\n        if self.variance_trend is not None and abs(self.variance_trend) > 0.1:\n            direction = \"increasing\" if self.variance_trend > 0 else \"decreasing\"\n            issues.append(f\"Gradient variance {direction} (trend: {self.variance_trend:.3f})\")\n        \n        return len(issues) == 0, issues\n\n\nclass EnhancedGradientMonitor:\n    \"\"\"\n    Enhanced gradient health monitoring with advanced metrics from CLAUDE.md.\n    \"\"\"\n    \n    def __init__(self, window_size: int = 100, track_layers: bool = True):\n        \"\"\"\n        Initialize gradient monitor.\n        \n        Args:\n            window_size: Number of recent measurements to track\n            track_layers: Whether to track layer-wise metrics\n        \"\"\"\n        self.window_size = window_size\n        self.track_layers = track_layers\n        self.history: List[GradientMetrics] = []\n        self.variance_history: List[float] = []\n        \n    def compute_metrics(self, grads: Any, prefix: str = \"\") -> GradientMetrics:\n        \"\"\"\n        Compute comprehensive gradient metrics.\n        \n        Args:\n            grads: PyTree of gradients\n            prefix: Optional prefix for nested structures\n            \n        Returns:\n            GradientMetrics object with all computed metrics\n        \"\"\"\n        # Flatten gradients and get structure info\n        flat_grads, tree_def = tree_util.tree_flatten_with_path(grads)\n        \n        # Separate by layers if tracking enabled\n        layer_grads = {}\n        layer_norms = {}\n        \n        for path, grad in flat_grads:\n            if grad is None:\n                continue\n                \n            # Extract layer name from path\n            layer_name = self._path_to_layer_name(path, prefix)\n            \n            if self.track_layers:\n                if layer_name not in layer_grads:\n                    layer_grads[layer_name] = []\n                layer_grads[layer_name].append(grad.flatten())\n                \n        # Compute layer norms\n        for layer_name, grads_list in layer_grads.items():\n            layer_grad = jnp.concatenate(grads_list)\n            layer_norms[layer_name] = float(jnp.linalg.norm(layer_grad))\n        \n        # Get all gradients as single array\n        all_grads = jnp.concatenate([g.flatten() for _, g in flat_grads if g is not None])\n        \n        # Compute norm ratio between first and last layers\n        layer_names = list(layer_norms.keys())\n        if len(layer_names) >= 2:\n            first_norm = layer_norms[layer_names[0]]\n            last_norm = layer_norms[layer_names[-1]]\n            norm_ratio = last_norm / (first_norm + 1e-8)\n        else:\n            norm_ratio = 1.0\n        \n        # Signal-to-total-variance ratio (prevents cancellation)\n        batch_dim = 0  # Assuming first dimension is batch\n        if all_grads.ndim > 1:\n            mean_over_batch = jnp.mean(all_grads, axis=batch_dim)\n            var_of_mean = jnp.var(mean_over_batch)\n            mean_of_var = jnp.mean(jnp.var(all_grads, axis=batch_dim))\n            signal_to_total_variance = var_of_mean / (mean_of_var + 1e-8)\n        else:\n            signal_to_total_variance = jnp.var(all_grads) / (jnp.var(all_grads) + 1e-8)\n        \n        # Variance of absolute gradients (prevents positive/negative cancellation)\n        variance_abs_gradients = float(jnp.var(jnp.abs(all_grads)))\n        \n        # Gradient sparsity\n        gradient_sparsity = float(jnp.mean(jnp.abs(all_grads) < 1e-6))\n        \n        # Store variance for trend analysis\n        current_variance = float(jnp.var(all_grads))\n        self.variance_history.append(current_variance)\n        if len(self.variance_history) > self.window_size:\n            self.variance_history.pop(0)\n        \n        # Compute variance trend if enough history\n        variance_trend = None\n        if len(self.variance_history) >= 10:\n            # Simple linear regression on log variance\n            x = np.arange(len(self.variance_history))\n            y = np.log(np.array(self.variance_history) + 1e-8)\n            variance_trend = float(np.polyfit(x, y, 1)[0])\n        \n        metrics = GradientMetrics(\n            norm_ratio=float(norm_ratio),\n            signal_to_total_variance=float(signal_to_total_variance),\n            variance_abs_gradients=variance_abs_gradients,\n            gradient_sparsity=gradient_sparsity,\n            variance_trend=variance_trend,\n            layer_norms=layer_norms\n        )\n        \n        # Add to history\n        self.history.append(metrics)\n        if len(self.history) > self.window_size:\n            self.history.pop(0)\n        \n        return metrics\n    \n    def _path_to_layer_name(self, path: Tuple[Any, ...], prefix: str) -> str:\n        \"\"\"Convert JAX path to readable layer name.\"\"\"\n        parts = []\n        if prefix:\n            parts.append(prefix)\n            \n        for key in path:\n            # Handle both old and new JAX path formats\n            if hasattr(key, 'key'):\n                parts.append(str(key.key))\n            else:\n                parts.append(str(key))\n                \n        return \".\".join(parts) if parts else \"root\"\n    \n    def get_summary_stats(self) -> Dict[str, float]:\n        \"\"\"Get summary statistics over recent history.\"\"\"\n        if not self.history:\n            return {}\n        \n        recent = self.history[-20:]  # Last 20 measurements\n        \n        summary = {\n            'mean_norm_ratio': np.mean([m.norm_ratio for m in recent]),\n            'std_norm_ratio': np.std([m.norm_ratio for m in recent]),\n            'mean_stv': np.mean([m.signal_to_total_variance for m in recent]),\n            'mean_sparsity': np.mean([m.gradient_sparsity for m in recent]),\n            'health_rate': np.mean([m.is_healthy()[0] for m in recent])\n        }\n        \n        return summary\n    \n    def plot_history(self, save_path: Optional[str] = None):\n        \"\"\"Plot gradient health metrics over time.\"\"\"\n        if not self.history:\n            return\n        \n        import matplotlib.pyplot as plt\n        \n        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n        fig.suptitle('Gradient Health Monitoring')\n        \n        # Norm ratios\n        norm_ratios = [m.norm_ratio for m in self.history]\n        axes[0, 0].plot(norm_ratios)\n        axes[0, 0].axhline(y=0.1, color='r', linestyle='--', alpha=0.5)\n        axes[0, 0].axhline(y=10, color='r', linestyle='--', alpha=0.5)\n        axes[0, 0].set_ylabel('Norm Ratio')\n        axes[0, 0].set_yscale('log')\n        axes[0, 0].set_title('Gradient Flow (First/Last Layer)')\n        \n        # Signal-to-total-variance\n        stvs = [m.signal_to_total_variance for m in self.history]\n        axes[0, 1].plot(stvs)\n        axes[0, 1].axhline(y=0.01, color='r', linestyle='--', alpha=0.5)\n        axes[0, 1].set_ylabel('STV Ratio')\n        axes[0, 1].set_yscale('log')\n        axes[0, 1].set_title('Signal-to-Total-Variance')\n        \n        # Variance of absolute gradients\n        var_abs = [m.variance_abs_gradients for m in self.history]\n        axes[1, 0].plot(var_abs)\n        axes[1, 0].set_ylabel('Variance')\n        axes[1, 0].set_yscale('log')\n        axes[1, 0].set_title('Variance of |Gradients|')\n        \n        # Sparsity\n        sparsity = [m.gradient_sparsity for m in self.history]\n        axes[1, 1].plot(sparsity)\n        axes[1, 1].axhline(y=0.9, color='r', linestyle='--', alpha=0.5)\n        axes[1, 1].set_ylabel('Sparsity')\n        axes[1, 1].set_title('Gradient Sparsity')\n        \n        for ax in axes.flat:\n            ax.set_xlabel('Training Step')\n            ax.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=150)\n        else:\n            plt.show()\n\n\nclass GradientClipTracker:\n    \"\"\"Track gradient clipping statistics.\"\"\"\n    \n    def __init__(self):\n        self.clip_history = []\n        self.norm_history = []\n    \n    def track_clip(self, global_norm: float, max_norm: float, clipped: bool):\n        \"\"\"Track a gradient clipping event.\"\"\"\n        self.norm_history.append(float(global_norm))\n        self.clip_history.append(clipped)\n        \n        # Keep only recent history\n        if len(self.norm_history) > 1000:\n            self.norm_history.pop(0)\n            self.clip_history.pop(0)\n    \n    def get_clip_rate(self, window: int = 100) -> float:\n        \"\"\"Get recent clipping rate.\"\"\"\n        if len(self.clip_history) < window:\n            recent = self.clip_history\n        else:\n            recent = self.clip_history[-window:]\n        \n        return np.mean(recent) if recent else 0.0\n    \n    def get_norm_stats(self) -> Dict[str, float]:\n        \"\"\"Get gradient norm statistics.\"\"\"\n        if not self.norm_history:\n            return {}\n        \n        return {\n            'mean_norm': np.mean(self.norm_history),\n            'std_norm': np.std(self.norm_history),\n            'max_norm': np.max(self.norm_history),\n            'p95_norm': np.percentile(self.norm_history, 95)\n        }\n\n\ndef apply_global_gradient_clip(\n    grads: Any,\n    max_norm: float = 10.0,\n    clip_tracker: Optional[GradientClipTracker] = None\n) -> Tuple[Any, bool]:\n    \"\"\"\n    Apply global gradient norm clipping with tracking.\n    \n    Args:\n        grads: PyTree of gradients\n        max_norm: Maximum allowed global norm\n        clip_tracker: Optional tracker for clipping statistics\n        \n    Returns:\n        Clipped gradients and whether clipping occurred\n    \"\"\"\n    # Compute global norm\n    leaves = tree_util.tree_leaves(grads)\n    global_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in leaves if g is not None))\n    \n    # Compute clip factor\n    clip_factor = jnp.minimum(1.0, max_norm / (global_norm + 1e-8))\n    clip_occurred = global_norm > max_norm\n    \n    # Track if requested\n    if clip_tracker is not None:\n        clip_tracker.track_clip(float(global_norm), max_norm, bool(clip_occurred))\n    \n    # Apply clipping\n    clipped_grads = tree_util.tree_map(lambda g: g * clip_factor if g is not None else None, grads)\n    \n    return clipped_grads, bool(clip_occurred)\n\n\nif __name__ == \"__main__\":\n    # Example usage with synthetic gradients\n    print(\"Testing enhanced gradient monitoring...\")\n    \n    # Create monitor\n    monitor = EnhancedGradientMonitor()\n    \n    # Simulate training with varying gradient health\n    key = jax.random.PRNGKey(42)\n    \n    for step in range(200):\n        key, subkey = jax.random.split(key)\n        \n        # Simulate gradients with varying properties\n        if step < 50:\n            # Healthy gradients\n            scale = 1.0\n        elif step < 100:\n            # Vanishing gradients\n            scale = 0.001 ** (step / 100)\n        elif step < 150:\n            # Exploding gradients\n            scale = 10 ** ((step - 100) / 50)\n        else:\n            # Recovery\n            scale = 1.0\n        \n        # Create synthetic gradient structure\n        grads = {\n            'layer1': {\n                'weight': scale * jax.random.normal(subkey, (32, 16)),\n                'bias': scale * jax.random.normal(jax.random.split(subkey)[0], (32,))\n            },\n            'layer2': {\n                'weight': scale * 0.5 * jax.random.normal(jax.random.split(subkey)[1], (16, 8)),\n                'bias': scale * 0.5 * jax.random.normal(jax.random.split(subkey, 3)[2], (16,))\n            }\n        }\n        \n        # Compute metrics\n        metrics = monitor.compute_metrics(grads)\n        \n        if step % 50 == 0:\n            is_healthy, issues = metrics.is_healthy()\n            print(f\"\\nStep {step}:\")\n            print(f\"  Healthy: {is_healthy}\")\n            if not is_healthy:\n                print(f\"  Issues: {', '.join(issues)}\")\n            print(f\"  Norm ratio: {metrics.norm_ratio:.3f}\")\n            print(f\"  STV: {metrics.signal_to_total_variance:.3e}\")\n            print(f\"  Sparsity: {metrics.gradient_sparsity:.1%}\")\n    \n    # Summary statistics\n    print(\"\\nSummary statistics:\")\n    for key, value in monitor.get_summary_stats().items():\n        print(f\"  {key}: {value:.3f}\")\n    \n    # Save gradient health plot\n    monitor.plot_history(save_path=\"gradient_health.png\")\n    print(\"\\nGradient health plot saved to gradient_health.png\")",
  "crypto_dp/monitoring/__init__.py": "\"\"\"Monitoring infrastructure for E2E-DP systems.\"\"\"\n\nfrom .gradient_health import (\n    GradientMetrics,\n    EnhancedGradientMonitor,\n    GradientClipTracker,\n    apply_global_gradient_clip\n)\n\n__all__ = [\n    'GradientMetrics',\n    'EnhancedGradientMonitor', \n    'GradientClipTracker',\n    'apply_global_gradient_clip'\n]",
  "crypto_dp/benchmarks/__init__.py": "\"\"\"Micro-benchmarks for E2E-DP validation.\"\"\"\n\nfrom .pendulum import (\n    PendulumDynamics,\n    DifferentiableController,\n    GradientHealthMonitor,\n    train_pendulum_controller\n)\n\n__all__ = [\n    'PendulumDynamics',\n    'DifferentiableController',\n    'GradientHealthMonitor',\n    'train_pendulum_controller'\n]",
  "crypto_dp/benchmarks/pendulum.py": "\"\"\"\nDifferentiable pendulum control micro-benchmark.\n\nThis module implements a simple pendulum control task to validate\ngradient flow through the E2E-DP pipeline before moving to complex\ntrading systems.\n\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom typing import Tuple, Dict, Any\nimport equinox as eqx\nimport optax\n\n\nclass PendulumDynamics(eqx.Module):\n    \"\"\"Differentiable pendulum dynamics with implicit integration.\"\"\"\n    \n    mass: float = 1.0\n    length: float = 1.0\n    gravity: float = 9.81\n    damping: float = 0.1\n    dt: float = 0.01\n    \n    def __call__(self, state: jnp.ndarray, control: jnp.ndarray) -> jnp.ndarray:\n        \"\"\"\n        Forward dynamics of pendulum.\n        \n        Args:\n            state: [theta, theta_dot] - angle and angular velocity\n            control: [u] - control torque\n        \n        Returns:\n            next_state: [theta_new, theta_dot_new]\n        \"\"\"\n        theta, theta_dot = state[0], state[1]\n        u = control[0]\n        \n        # Pendulum dynamics: theta_ddot = -(g/l)*sin(theta) - damping*theta_dot + u/(m*l^2)\n        theta_ddot = (\n            -(self.gravity / self.length) * jnp.sin(theta)\n            - self.damping * theta_dot\n            + u / (self.mass * self.length**2)\n        )\n        \n        # Semi-implicit Euler integration (more stable for oscillatory systems)\n        theta_dot_new = theta_dot + self.dt * theta_ddot\n        theta_new = theta + self.dt * theta_dot_new\n        \n        return jnp.array([theta_new, theta_dot_new])\n\n\nclass DifferentiableController(eqx.Module):\n    \"\"\"End-to-end differentiable controller.\"\"\"\n    \n    policy_network: eqx.nn.MLP\n    dynamics: PendulumDynamics\n    \n    def __init__(self, hidden_dims: Tuple[int, ...] = (32, 32), key=None):\n        if key is None:\n            key = jax.random.PRNGKey(42)\n        \n        # Neural network policy\n        self.policy_network = eqx.nn.MLP(\n            in_size=2,  # theta, theta_dot\n            out_size=1,  # control torque\n            width_size=hidden_dims[0],\n            depth=len(hidden_dims),\n            activation=jax.nn.tanh,\n            key=key\n        )\n        \n        self.dynamics = PendulumDynamics()\n    \n    def __call__(self, state: jnp.ndarray) -> jnp.ndarray:\n        \"\"\"Generate control from state.\"\"\"\n        return self.policy_network(state)\n    \n    def rollout(self, initial_state: jnp.ndarray, horizon: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        \"\"\"\n        Simulate trajectory with learned controller.\n        \n        Returns:\n            states: [horizon+1, 2] - trajectory of states\n            controls: [horizon, 1] - sequence of controls\n        \"\"\"\n        def step(carry, _):\n            state = carry\n            control = self(state)\n            next_state = self.dynamics(state, control)\n            return next_state, (state, control)\n        \n        final_state, (states, controls) = jax.lax.scan(\n            step, initial_state, jnp.arange(horizon)\n        )\n        \n        # Append final state\n        states = jnp.concatenate([states, final_state[None, :]], axis=0)\n        \n        return states, controls\n\n\ndef smooth_control_cost(controls: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Penalize control effort and roughness.\"\"\"\n    effort = jnp.mean(controls**2)\n    smoothness = jnp.mean((controls[1:] - controls[:-1])**2)\n    return effort + 0.1 * smoothness\n\n\ndef stabilization_loss(\n    controller: DifferentiableController,\n    initial_state: jnp.ndarray,\n    target_state: jnp.ndarray = None,\n    horizon: int = 100\n) -> jnp.ndarray:\n    \"\"\"\n    Loss function for pendulum stabilization task.\n    \n    Args:\n        controller: Differentiable controller\n        initial_state: Starting state\n        target_state: Desired final state (default: upright position)\n        horizon: Planning horizon\n    \n    Returns:\n        Scalar loss value\n    \"\"\"\n    if target_state is None:\n        target_state = jnp.array([0.0, 0.0])  # Upright, stationary\n    \n    states, controls = controller.rollout(initial_state, horizon)\n    \n    # State tracking loss (focus on final states)\n    state_errors = states - target_state[None, :]\n    tracking_loss = jnp.mean(state_errors[-20:]**2)  # Focus on last 20% of trajectory\n    \n    # Control regularization\n    control_loss = smooth_control_cost(controls)\n    \n    # Stability bonus (small velocities near target)\n    stability_loss = jnp.mean(jnp.abs(states[-20:, 1]))  # Angular velocity should be small\n    \n    return tracking_loss + 0.01 * control_loss + 0.1 * stability_loss\n\n\nclass GradientHealthMonitor:\n    \"\"\"Monitor gradient health metrics during training.\"\"\"\n    \n    def __init__(self):\n        self.history = {\n            'gradient_norms': [],\n            'gradient_variance': [],\n            'signal_to_noise': [],\n            'layer_ratios': []\n        }\n    \n    def compute_metrics(self, grads: Any) -> Dict[str, float]:\n        \"\"\"Compute gradient health metrics.\"\"\"\n        # Flatten all gradients\n        flat_grads, _ = jax.tree_util.tree_flatten(grads)\n        all_grads = jnp.concatenate([g.flatten() for g in flat_grads])\n        \n        # Basic statistics\n        grad_norm = jnp.linalg.norm(all_grads)\n        grad_mean = jnp.mean(all_grads)\n        grad_var = jnp.var(all_grads)\n        \n        # Signal-to-noise ratio\n        snr = jnp.abs(grad_mean) / (jnp.sqrt(grad_var) + 1e-8)\n        \n        # Layer-wise analysis\n        layer_norms = [jnp.linalg.norm(g.flatten()) for g in flat_grads]\n        if len(layer_norms) > 1:\n            # Ratio between first and last layer\n            layer_ratio = layer_norms[-1] / (layer_norms[0] + 1e-8)\n        else:\n            layer_ratio = 1.0\n        \n        metrics = {\n            'gradient_norm': float(grad_norm),\n            'gradient_variance': float(grad_var),\n            'signal_to_noise': float(snr),\n            'layer_ratio': float(layer_ratio)\n        }\n        \n        # Update history\n        for key, value in metrics.items():\n            if key in self.history:\n                self.history[key].append(value)\n        \n        return metrics\n    \n    def check_health(self, grads: Any) -> Tuple[bool, str]:\n        \"\"\"Check if gradients are healthy.\"\"\"\n        metrics = self.compute_metrics(grads)\n        \n        issues = []\n        \n        # Check for vanishing gradients\n        if metrics['gradient_norm'] < 1e-6:\n            issues.append(\"Vanishing gradients detected\")\n        \n        # Check for exploding gradients\n        if metrics['gradient_norm'] > 100:\n            issues.append(\"Exploding gradients detected\")\n        \n        # Check layer ratio\n        if metrics['layer_ratio'] < 0.01 or metrics['layer_ratio'] > 100:\n            issues.append(f\"Poor gradient flow between layers (ratio: {metrics['layer_ratio']:.2f})\")\n        \n        # Check SNR\n        if metrics['signal_to_noise'] < 0.1:\n            issues.append(\"Low gradient signal-to-noise ratio\")\n        \n        is_healthy = len(issues) == 0\n        message = \"Gradients healthy\" if is_healthy else \"; \".join(issues)\n        \n        return is_healthy, message\n\n\ndef train_step(\n    controller: DifferentiableController,\n    opt_state: Any,\n    optimizer: optax.GradientTransformation,\n    initial_state: jnp.ndarray,\n    target_state: jnp.ndarray,\n    horizon: int\n) -> Tuple[DifferentiableController, Any, float, Any]:\n    \"\"\"Single training step.\"\"\"\n    \n    # Compute loss and gradients\n    loss_fn = lambda ctrl: stabilization_loss(ctrl, initial_state, target_state, horizon)\n    loss, grads = eqx.filter_value_and_grad(loss_fn)(controller)\n    \n    # Update parameters\n    updates, opt_state = optimizer.update(grads, opt_state)\n    controller = eqx.apply_updates(controller, updates)\n    \n    return controller, opt_state, loss, grads\n\n\ndef train_pendulum_controller(\n    n_steps: int = 1000,\n    learning_rate: float = 1e-3,\n    horizon: int = 100,\n    seed: int = 42\n) -> Tuple[DifferentiableController, Dict[str, Any]]:\n    \"\"\"\n    Train pendulum controller and monitor gradient health.\n    \n    Returns:\n        Trained controller and training metrics\n    \"\"\"\n    key = jax.random.PRNGKey(seed)\n    \n    # Initialize controller\n    controller = DifferentiableController(hidden_dims=(32, 32), key=key)\n    \n    # Optimizer\n    optimizer = optax.adam(learning_rate)\n    opt_state = optimizer.init(eqx.filter(controller, eqx.is_array))\n    \n    # Gradient health monitor\n    monitor = GradientHealthMonitor()\n    \n    # Training metrics\n    losses = []\n    gradient_healths = []\n    \n    # Training loop\n    for step in range(n_steps):\n        # Random initial state (pendulum hanging down with small perturbation)\n        key, subkey = jax.random.split(key)\n        initial_state = jnp.array([\n            jnp.pi + 0.1 * jax.random.normal(subkey),  # Near bottom\n            0.1 * jax.random.normal(jax.random.split(subkey)[0])  # Small velocity\n        ])\n        \n        # Target: upright position\n        target_state = jnp.array([0.0, 0.0])\n        \n        # Training step\n        controller, opt_state, loss, grads = train_step(\n            controller, opt_state, optimizer, initial_state, target_state, horizon\n        )\n        \n        losses.append(float(loss))\n        \n        # Monitor gradient health\n        if step % 10 == 0:\n            is_healthy, message = monitor.check_health(grads)\n            gradient_healths.append(is_healthy)\n            \n            if step % 100 == 0:\n                metrics = monitor.compute_metrics(grads)\n                print(f\"Step {step}: Loss = {loss:.4f}, {message}\")\n                print(f\"  Gradient norm: {metrics['gradient_norm']:.2e}\")\n                print(f\"  Layer ratio: {metrics['layer_ratio']:.2f}\")\n                print(f\"  SNR: {metrics['signal_to_noise']:.2f}\")\n    \n    # Compile results\n    results = {\n        'losses': jnp.array(losses),\n        'gradient_history': monitor.history,\n        'health_percentage': jnp.mean(jnp.array(gradient_healths)) * 100\n    }\n    \n    return controller, results\n\n\nif __name__ == \"__main__\":\n    print(\"Running differentiable pendulum control benchmark...\")\n    \n    # Train controller\n    controller, results = train_pendulum_controller(n_steps=500)\n    \n    print(f\"\\nTraining completed!\")\n    print(f\"Final loss: {results['losses'][-1]:.4f}\")\n    print(f\"Gradient health: {results['health_percentage']:.1f}% of checks passed\")\n    \n    # Test learned controller\n    test_state = jnp.array([jnp.pi, 0.0])  # Start from bottom\n    states, controls = controller.rollout(test_state, horizon=200)\n    \n    print(f\"\\nTest rollout from bottom position:\")\n    print(f\"Initial state: ={test_state[0]:.2f}, _dot={test_state[1]:.2f}\")\n    print(f\"Final state: ={states[-1, 0]:.2f}, _dot={states[-1, 1]:.2f}\")\n    print(f\"Average control magnitude: {jnp.mean(jnp.abs(controls)):.2f}\")",
  "crypto_dp/graph/scaffold.py": "\"\"\"\nLatent graph scaffold for structured differentiable programming.\n\nThis module implements a differentiable graph structure that can learn\nlatent relationships between cryptocurrency assets and market factors.\nUses JAX for automatic differentiation and efficient computation.\n\"\"\"\n\nfrom typing import Tuple, Optional, Callable\nimport logging\n\nimport jax\nimport jax.numpy as jnp\nimport networkx as nx\nimport equinox as eqx\nimport optax\nfrom jax import grad, jit, vmap\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass LatentGraph(eqx.Module):\n    \"\"\"\n    Differentiable latent graph for crypto asset relationships.\n    \n    This module learns a structured representation of relationships between\n    crypto assets using a parameterized adjacency matrix that can be optimized\n    via gradient descent.\n    \"\"\"\n    \n    W: jnp.ndarray  # Adjacency weight matrix\n    n_factors: int\n    activation: Callable\n    \n    def __init__(\n        self,\n        n_factors: int,\n        activation: Callable = jax.nn.tanh,\n        key: Optional[jax.random.PRNGKey] = None\n    ):\n        \"\"\"\n        Initialize latent graph.\n        \n        Args:\n            n_factors: Number of latent factors/nodes\n            activation: Activation function for message passing\n            key: Random key for initialization\n        \"\"\"\n        if n_factors < 1:\n            raise ValueError(\"n_factors must be >= 1\")\n            \n        if key is None:\n            key = jax.random.PRNGKey(42)\n            \n        self.n_factors = n_factors\n        self.activation = activation\n        \n        # Initialize adjacency matrix with small random weights\n        self.W = jax.random.normal(key, (n_factors, n_factors)) * 0.1\n        \n    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n        \"\"\"\n        Forward pass through the latent graph.\n        \n        Args:\n            x: Input features [batch_size, n_factors] or [n_factors]\n        \n        Returns:\n            Graph-transformed features\n        \"\"\"\n        # Linear message passing with activation\n        if x.ndim == 1:\n            # Single sample\n            h = x @ self.W\n        else:\n            # Batch processing\n            h = jnp.einsum('bi,ij->bj', x, self.W)\n            \n        return self.activation(h)\n    \n    def forward_multi_step(self, x: jnp.ndarray, n_steps: int = 3) -> jnp.ndarray:\n        \"\"\"\n        Multi-step message passing through the graph.\n        \n        Args:\n            x: Input features\n            n_steps: Number of message passing steps\n        \n        Returns:\n            Features after n_steps of graph propagation\n        \"\"\"\n        h = x\n        for _ in range(n_steps):\n            h = self(h)\n        return h\n    \n    def get_adjacency_matrix(self) -> jnp.ndarray:\n        \"\"\"Get the current adjacency matrix.\"\"\"\n        return self.W\n    \n    def get_graph_structure(self, threshold: float = 0.1) -> nx.DiGraph:\n        \"\"\"\n        Convert learned weights to a NetworkX graph for visualization.\n        \n        Args:\n            threshold: Minimum weight magnitude to include edge\n        \n        Returns:\n            NetworkX directed graph\n        \"\"\"\n        G = nx.DiGraph()\n        G.add_nodes_from(range(self.n_factors))\n        \n        weights = self.W\n        for i in range(self.n_factors):\n            for j in range(self.n_factors):\n                if abs(weights[i, j]) > threshold:\n                    G.add_edge(i, j, weight=float(weights[i, j]))\n        \n        return G\n\n\ndef bic_loss(\n    model: LatentGraph,\n    x: jnp.ndarray,\n    target: jnp.ndarray,\n    lambda_reg: float = 1e-2\n) -> jnp.ndarray:\n    \"\"\"\n    BIC-like loss function for graph structure learning.\n    \n    Combines prediction accuracy with complexity penalty to encourage\n    sparse, interpretable graph structures.\n    \n    Args:\n        model: LatentGraph model\n        x: Input features\n        target: Target values\n        lambda_reg: Regularization strength\n    \n    Returns:\n        Scalar loss value\n    \"\"\"\n    # Forward pass\n    preds = model(x)\n    \n    # Prediction loss (MSE)\n    mse = jnp.mean((preds - target) ** 2)\n    \n    # Complexity penalty (L1 regularization on adjacency matrix)\n    complexity = lambda_reg * jnp.sum(jnp.abs(model.W))\n    \n    # BIC-like penalty (encourages sparsity)\n    n_edges = jnp.sum(jnp.abs(model.W) > 1e-6)\n    bic_penalty = 0.5 * n_edges * jnp.log(x.shape[0])  # log(n_samples)\n    \n    return mse + complexity + lambda_reg * bic_penalty\n\n\ndef spectral_regularization(model: LatentGraph, alpha: float = 1e-3) -> jnp.ndarray:\n    \"\"\"\n    Spectral regularization to encourage stable graph dynamics.\n    \n    Args:\n        model: LatentGraph model\n        alpha: Regularization strength\n    \n    Returns:\n        Regularization term\n    \"\"\"\n    # Compute spectral radius (largest eigenvalue magnitude)\n    eigenvals = jnp.linalg.eigvals(model.W)\n    spectral_radius = jnp.max(jnp.abs(eigenvals))\n    \n    # Penalty for spectral radius > 1 (instability)\n    return alpha * jnp.maximum(0.0, spectral_radius - 1.0) ** 2\n\n\n@jit\ndef graph_step(\n    model: LatentGraph,\n    opt_state: optax.OptState,\n    x: jnp.ndarray,\n    target: jnp.ndarray,\n    optimizer: optax.GradientTransformation,\n    lambda_reg: float = 1e-2\n) -> Tuple[LatentGraph, optax.OptState, jnp.ndarray]:\n    \"\"\"\n    Single optimization step for the latent graph.\n    \n    Args:\n        model: Current LatentGraph model\n        opt_state: Current optimizer state\n        x: Input features\n        target: Target values\n        optimizer: Optax optimizer\n        lambda_reg: Regularization strength\n    \n    Returns:\n        Updated model, updated optimizer state, and loss value\n    \"\"\"\n    def loss_fn(model):\n        return bic_loss(model, x, target, lambda_reg) + spectral_regularization(model)\n    \n    loss, grads = eqx.filter_value_and_grad(loss_fn)(model)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    model = eqx.apply_updates(model, updates)\n    \n    return model, opt_state, loss\n\n\ndef train_graph(\n    model: LatentGraph,\n    x_train: jnp.ndarray,\n    y_train: jnp.ndarray,\n    x_val: Optional[jnp.ndarray] = None,\n    y_val: Optional[jnp.ndarray] = None,\n    n_epochs: int = 1000,\n    learning_rate: float = 1e-3,\n    lambda_reg: float = 1e-2,\n    patience: int = 50,\n    verbose: bool = True\n) -> Tuple[LatentGraph, dict]:\n    \"\"\"\n    Train the latent graph model.\n    \n    Args:\n        model: Initial LatentGraph model\n        x_train: Training features\n        y_train: Training targets\n        x_val: Validation features (optional)\n        y_val: Validation targets (optional)\n        n_epochs: Maximum number of training epochs\n        learning_rate: Learning rate\n        lambda_reg: Regularization strength\n        patience: Early stopping patience\n        verbose: Whether to print training progress\n    \n    Returns:\n        Trained model and training history\n    \"\"\"\n    # Initialize optimizer\n    optimizer = optax.sgd(learning_rate)\n    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n    \n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'spectral_radius': []\n    }\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_model = model\n    \n    for epoch in range(n_epochs):\n        # Training step\n        model, opt_state, train_loss = graph_step(\n            model, opt_state, x_train, y_train, optimizer, lambda_reg\n        )\n        \n        history['train_loss'].append(float(train_loss))\n        \n        # Compute spectral radius for monitoring\n        eigenvals = jnp.linalg.eigvals(model.W)\n        spectral_radius = float(jnp.max(jnp.abs(eigenvals)))\n        history['spectral_radius'].append(spectral_radius)\n        \n        # Validation\n        if x_val is not None and y_val is not None:\n            val_loss = bic_loss(model, x_val, y_val, lambda_reg)\n            history['val_loss'].append(float(val_loss))\n            \n            # Early stopping\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_model = model\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= patience:\n                if verbose:\n                    logger.info(f\"Early stopping at epoch {epoch}\")\n                break\n        else:\n            best_model = model\n        \n        # Logging\n        if verbose and epoch % 100 == 0:\n            if x_val is not None:\n                logger.info(\n                    f\"Epoch {epoch}: train_loss={train_loss:.6f}, \"\n                    f\"val_loss={val_loss:.6f}, spectral_radius={spectral_radius:.4f}\"\n                )\n            else:\n                logger.info(\n                    f\"Epoch {epoch}: train_loss={train_loss:.6f}, \"\n                    f\"spectral_radius={spectral_radius:.4f}\"\n                )\n    \n    return best_model, history\n\n\ndef create_crypto_factor_graph(\n    n_assets: int,\n    n_market_factors: int = 5,\n    key: Optional[jax.random.PRNGKey] = None\n) -> LatentGraph:\n    \"\"\"\n    Create a latent graph specifically for crypto asset relationships.\n    \n    Args:\n        n_assets: Number of crypto assets\n        n_market_factors: Number of market-wide factors (e.g., BTC dominance, DeFi, etc.)\n        key: Random key for initialization\n    \n    Returns:\n        Initialized LatentGraph for crypto assets\n    \"\"\"\n    total_factors = n_assets + n_market_factors\n    \n    if key is None:\n        key = jax.random.PRNGKey(42)\n    \n    model = LatentGraph(total_factors, key=key)\n    \n    logger.info(\n        f\"Created crypto factor graph with {n_assets} assets \"\n        f\"and {n_market_factors} market factors\"\n    )\n    \n    return model\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    logging.basicConfig(level=logging.INFO)\n    \n    # Create sample data\n    key = jax.random.PRNGKey(42)\n    n_samples, n_factors = 1000, 10\n    \n    x_data = jax.random.normal(key, (n_samples, n_factors))\n    \n    # Create synthetic targets with some structure\n    true_W = jax.random.normal(jax.random.split(key)[0], (n_factors, n_factors)) * 0.1\n    y_data = x_data @ true_W + 0.1 * jax.random.normal(jax.random.split(key)[1], (n_samples, n_factors))\n    \n    # Initialize and train model\n    model = LatentGraph(n_factors, key=key)\n    trained_model, history = train_graph(model, x_data, y_data, n_epochs=500)\n    \n    logger.info(\"Training completed successfully\")",
  "crypto_dp/graph/__init__.py": "# Latent graph scaffold module",
  "crypto_dp/rl/__init__.py": "# Reinforcement learning agent loop module",
  "crypto_dp/rl/agent.py": "\"\"\"\nReinforcement learning agent for crypto trading.\n\nThis module implements the core RL loop for training and deploying\ncrypto trading agents with built-in risk management and execution\nsimulation capabilities.\n\"\"\"\n\nfrom typing import Dict, Any, Tuple, Optional, List, NamedTuple\nimport logging\nfrom enum import Enum\nfrom dataclasses import dataclass\nimport time\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\nfrom jax import jit, vmap\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass TradingAction(NamedTuple):\n    \"\"\"Trading action with position sizes and metadata.\"\"\"\n    positions: jnp.ndarray  # Position sizes for each asset\n    confidence: float       # Confidence in the action\n    timestamp: float       # Action timestamp\n\n\nclass MarketState(NamedTuple):\n    \"\"\"Market state representation.\"\"\"\n    prices: jnp.ndarray          # Current asset prices\n    features: jnp.ndarray        # Market features\n    portfolio: jnp.ndarray       # Current portfolio weights\n    cash: float                  # Available cash\n    timestamp: float             # State timestamp\n\n\nclass RiskLevel(Enum):\n    \"\"\"Risk level indicators for position sizing.\"\"\"\n    LOW = 0.5\n    MEDIUM = 1.0\n    HIGH = 1.5\n    CRITICAL = 2.0\n\n\n@dataclass\nclass TradingEnvironment:\n    \"\"\"\n    Crypto trading environment for RL training.\n    \n    Simulates realistic trading conditions including:\n    - Transaction costs\n    - Slippage\n    - Latency\n    - Market impact\n    \"\"\"\n    \n    symbols: List[str]\n    initial_cash: float = 100000.0\n    transaction_cost: float = 0.001  # 0.1%\n    slippage_rate: float = 0.0005    # 0.05%\n    max_position_size: float = 0.2   # 20% max per asset\n    lookback_window: int = 100\n    \n    def __post_init__(self):\n        if not self.symbols:\n            raise ValueError(\"At least one symbol required\")\n        self.n_assets = len(self.symbols)\n        self.reset()\n    \n    def reset(self) -> MarketState:\n        \"\"\"Reset environment to initial state.\"\"\"\n        self.cash = self.initial_cash\n        self.positions = jnp.zeros(self.n_assets)\n        self.portfolio_value = self.initial_cash\n        self.step_count = 0\n        \n        return self._get_state()\n    \n    def _get_state(self) -> MarketState:\n        \"\"\"Get current market state.\"\"\"\n        # Placeholder - in practice, would fetch real market data\n        prices = jnp.ones(self.n_assets)  # Dummy prices\n        features = jnp.zeros(20)          # Dummy features\n        \n        # Calculate portfolio weights, handling zero positions case\n        total_abs_position = jnp.sum(jnp.abs(self.positions))\n        portfolio_weights = jnp.where(\n            total_abs_position < 1e-8,\n            jnp.zeros_like(self.positions),  # All zeros if no positions\n            self.positions / total_abs_position\n        )\n        \n        return MarketState(\n            prices=prices,\n            features=features,\n            portfolio=portfolio_weights,\n            cash=self.cash,\n            timestamp=float(self.step_count)  # Use step count for deterministic timestamps\n        )\n    \n    def step(self, action: TradingAction) -> Tuple[MarketState, float, bool, Dict[str, Any]]:\n        \"\"\"\n        Execute trading action and return new state.\n        \n        Args:\n            action: Trading action to execute\n        \n        Returns:\n            new_state, reward, done, info\n        \"\"\"\n        # Simulate transaction costs and slippage\n        new_positions = action.positions\n        position_change = jnp.abs(new_positions - self.positions)\n        \n        # Calculate transaction costs\n        transaction_cost = jnp.sum(position_change) * self.transaction_cost\n        \n        # Calculate slippage (simplified)\n        slippage = jnp.sum(position_change) * self.slippage_rate\n        \n        # Store previous portfolio value for P&L calculation\n        prev_portfolio_value = self.portfolio_value\n        \n        # Update positions\n        self.positions = new_positions\n        \n        # Simulate price changes (simple GBM for now)\n        # In practice, this would use real market data\n        # Use deterministic seed based on step count for reproducibility\n        price_returns = jax.random.normal(\n            jax.random.PRNGKey(42 + self.step_count), \n            (self.n_assets,)\n        ) * 0.01  # 1% daily volatility\n        \n        # Calculate portfolio P&L\n        portfolio_pnl = jnp.sum(self.positions * price_returns) * prev_portfolio_value\n        \n        # Update portfolio value\n        self.portfolio_value = prev_portfolio_value + portfolio_pnl - transaction_cost - slippage\n        \n        # Calculate reward: P&L minus transaction costs\n        reward = portfolio_pnl - transaction_cost - slippage\n        \n        # Update cash (simplified)\n        self.cash -= transaction_cost + slippage\n        \n        self.step_count += 1\n        \n        # Environment termination conditions\n        done = (\n            self.step_count >= 1000 or  # Max steps\n            self.cash < 0 or            # Margin call\n            jnp.any(jnp.abs(self.positions) > self.max_position_size)  # Position limit\n        )\n        \n        info = {\n            'transaction_cost': transaction_cost,\n            'slippage': slippage,\n            'portfolio_value': self.portfolio_value,\n            'step_count': self.step_count\n        }\n        \n        return self._get_state(), float(reward), done, info\n\n\nclass CryptoTradingAgent(eqx.Module):\n    \"\"\"\n    Deep RL agent for crypto trading using JAX/Equinox.\n    \"\"\"\n    \n    policy_network: eqx.nn.MLP\n    value_network: eqx.nn.MLP\n    risk_module: eqx.nn.MLP\n    \n    def __init__(\n        self,\n        state_dim: int,\n        action_dim: int,\n        hidden_dims: Tuple[int, ...] = (256, 128, 64),\n        key: Optional[jax.random.PRNGKey] = None\n    ):\n        \"\"\"\n        Initialize crypto trading agent.\n        \n        Args:\n            state_dim: Dimension of state space\n            action_dim: Dimension of action space (number of assets)\n            hidden_dims: Hidden layer dimensions\n            key: Random key for initialization\n        \"\"\"\n        if key is None:\n            key = jax.random.PRNGKey(42)\n        \n        keys = jax.random.split(key, 3)\n        \n        # Policy network (outputs raw position scores)\n        self.policy_network = eqx.nn.MLP(\n            in_size=state_dim,\n            out_size=action_dim,\n            width_size=hidden_dims[0],\n            depth=len(hidden_dims),\n            key=keys[0]\n        )\n        \n        # Value network (state value estimation)\n        self.value_network = eqx.nn.MLP(\n            in_size=state_dim,\n            out_size=1,\n            width_size=hidden_dims[0] // 2,\n            depth=len(hidden_dims) - 1,\n            key=keys[1]\n        )\n        \n        # Risk assessment module\n        self.risk_module = eqx.nn.MLP(\n            in_size=state_dim,\n            out_size=4,  # Risk scores for different factors\n            width_size=hidden_dims[0] // 4,\n            depth=2,\n            key=keys[2]\n        )\n    \n    def __call__(self, state: jnp.ndarray) -> TradingAction:\n        \"\"\"Generate trading action from current state.\"\"\"\n        return self.get_action(state)\n    \n    def get_action(\n        self,\n        state: jnp.ndarray,\n        deterministic: bool = False,\n        key: Optional[jax.random.PRNGKey] = None\n    ) -> TradingAction:\n        \"\"\"\n        Generate trading action with risk-adjusted position sizing.\n        \n        Args:\n            state: Current market state\n            deterministic: Whether to use deterministic policy\n            key: Random key for stochastic actions\n        \n        Returns:\n            Trading action\n        \"\"\"\n        # Get raw policy scores\n        raw_scores = self.policy_network(state)\n        \n        # Risk assessment\n        risk_scores = self.risk_module(state)\n        risk_level = jnp.mean(jnp.abs(risk_scores))\n        \n        # Risk-adjusted position sizing\n        max_position = 0.2 / (1.0 + risk_level)  # Lower max when risk is high\n        \n        if deterministic:\n            # Deterministic action (for evaluation)\n            positions = jnp.tanh(raw_scores) * max_position\n        else:\n            # Stochastic action (for exploration)\n            if key is None:\n                # Use a deterministic fallback based on state hash for reproducibility\n                state_hash = hash(tuple(state.flatten())) % (2**31)\n                key = jax.random.PRNGKey(state_hash)\n            \n            noise = jax.random.normal(key, raw_scores.shape) * 0.1\n            noisy_scores = raw_scores + noise\n            positions = jnp.tanh(noisy_scores) * max_position\n        \n        # Ensure positions sum to approximately zero (market neutral)\n        positions = positions - jnp.mean(positions)\n        \n        # Calculate confidence based on score magnitude\n        confidence = float(jnp.mean(jnp.abs(raw_scores)))\n        \n        return TradingAction(\n            positions=positions,\n            confidence=confidence,\n            timestamp=0.0  # Use deterministic timestamp for reproducibility\n        )\n    \n    def get_value(self, state: jnp.ndarray) -> float:\n        \"\"\"Estimate state value.\"\"\"\n        value = self.value_network(state)\n        return float(value.squeeze())\n\n\nclass RiskManager:\n    \"\"\"\n    Risk management system with configurable limits and kill switches.\n    \"\"\"\n    \n    def __init__(\n        self,\n        max_drawdown: float = 0.08,\n        max_daily_loss: float = 0.02,\n        max_position_size: float = 0.2,\n        max_leverage: float = 2.0,\n        volatility_threshold: float = 2.0\n    ):\n        self.max_drawdown = max_drawdown\n        self.max_daily_loss = max_daily_loss\n        self.max_position_size = max_position_size\n        self.max_leverage = max_leverage\n        self.volatility_threshold = volatility_threshold\n        \n        # Risk state tracking\n        self.peak_portfolio_value = 100000.0\n        self.daily_start_value = 100000.0\n        self.last_reset_time = 0.0  # Use deterministic reset time\n    \n    def check_risk_limits(\n        self,\n        action: TradingAction,\n        portfolio_value: float,\n        volatility: float\n    ) -> Tuple[bool, List[str], TradingAction]:\n        \"\"\"\n        Check if action violates risk limits.\n        \n        Args:\n            action: Proposed trading action\n            portfolio_value: Current portfolio value\n            volatility: Current portfolio volatility\n        \n        Returns:\n            (is_safe, warnings, modified_action)\n        \"\"\"\n        warnings = []\n        modified_positions = action.positions.copy()\n        \n        # Update peak value\n        self.peak_portfolio_value = max(self.peak_portfolio_value, portfolio_value)\n        \n        # Check drawdown\n        current_drawdown = (self.peak_portfolio_value - portfolio_value) / self.peak_portfolio_value\n        if current_drawdown > self.max_drawdown:\n            warnings.append(f\"Max drawdown exceeded: {current_drawdown:.2%}\")\n            # Force flat positions\n            modified_positions = jnp.zeros_like(modified_positions)\n        \n        # Check daily loss\n        daily_loss = (self.daily_start_value - portfolio_value) / self.daily_start_value\n        if daily_loss > self.max_daily_loss:\n            warnings.append(f\"Max daily loss exceeded: {daily_loss:.2%}\")\n            # Reduce position sizes\n            modified_positions *= 0.5\n        \n        # Check position sizes\n        max_pos = jnp.max(jnp.abs(modified_positions))\n        if max_pos > self.max_position_size:\n            warnings.append(f\"Position size too large: {max_pos:.2%}\")\n            # Scale down positions\n            scale_factor = self.max_position_size / max_pos\n            modified_positions *= scale_factor\n        \n        # Check leverage (sum of absolute position weights)\n        leverage_ratio = jnp.sum(jnp.abs(modified_positions))\n        if leverage_ratio > self.max_leverage:\n            warnings.append(f\"Leverage too high: {leverage_ratio:.2f}x\")\n            # Scale down to max leverage\n            modified_positions *= self.max_leverage / leverage_ratio\n        \n        # Check volatility\n        if volatility > self.volatility_threshold:\n            warnings.append(f\"High volatility detected: {volatility:.2f}\")\n            # Reduce position sizes in high volatility\n            modified_positions *= 0.7\n        \n        # Check for daily reset\n        current_time = time.time()\n        if current_time - self.last_reset_time > 24 * 3600:  # 24 hours\n            self.daily_start_value = portfolio_value\n            self.last_reset_time = current_time\n        \n        is_safe = len(warnings) == 0\n        \n        modified_action = TradingAction(\n            positions=modified_positions,\n            confidence=action.confidence * (0.5 if warnings else 1.0),\n            timestamp=action.timestamp\n        )\n        \n        return is_safe, warnings, modified_action\n\n\ndef train_agent(\n    agent: CryptoTradingAgent,\n    env: TradingEnvironment,\n    n_episodes: int = 1000,\n    learning_rate: float = 3e-4,\n    gamma: float = 0.99,\n    verbose: bool = True\n) -> Tuple[CryptoTradingAgent, Dict[str, List[float]]]:\n    \"\"\"\n    Train the crypto trading agent using a simple policy gradient method.\n    \n    Args:\n        agent: CryptoTradingAgent to train\n        env: Trading environment\n        n_episodes: Number of training episodes\n        learning_rate: Learning rate for optimization\n        gamma: Discount factor\n        verbose: Whether to print training progress\n    \n    Returns:\n        Trained agent and training history\n    \"\"\"\n    optimizer = optax.adam(learning_rate)\n    opt_state = optimizer.init(agent)\n    \n    history = {\n        'episode_rewards': [],\n        'episode_lengths': [],\n        'policy_losses': [],\n        'value_losses': []\n    }\n    \n    for episode in range(n_episodes):\n        # Reset environment\n        state = env.reset()\n        episode_reward = 0.0\n        episode_length = 0\n        \n        # Collect episode data\n        states, actions, rewards = [], [], []\n        \n        done = False\n        while not done:\n            # Get current state as array\n            state_array = jnp.concatenate([\n                state.prices,\n                state.features,\n                state.portfolio,\n                jnp.array([state.cash])\n            ])\n            \n            # Get action from agent\n            action = agent.get_action(state_array, deterministic=False)\n            \n            # Take step in environment\n            next_state, reward, done, info = env.step(action)\n            \n            # Store experience\n            states.append(state_array)\n            actions.append(action.positions)\n            rewards.append(reward)\n            \n            # Update for next iteration\n            state = next_state\n            episode_reward += reward\n            episode_length += 1\n        \n        # Convert to arrays\n        states = jnp.stack(states)\n        actions = jnp.stack(actions)\n        rewards = jnp.array(rewards)\n        \n        # Compute returns (simplified)\n        returns = jnp.zeros_like(rewards)\n        discounted_sum = 0.0\n        for t in range(len(rewards) - 1, -1, -1):\n            discounted_sum = rewards[t] + gamma * discounted_sum\n            returns = returns.at[t].set(discounted_sum)\n        \n        # Policy gradient update (simplified)\n        def policy_loss_fn(agent):\n            values = vmap(agent.get_value)(states)\n            advantages = returns - values\n            \n            # Policy loss (simplified REINFORCE)\n            policy_scores = vmap(agent.policy_network)(states)\n            log_probs = -0.5 * jnp.sum((policy_scores - actions) ** 2, axis=1)\n            policy_loss = -jnp.mean(log_probs * advantages)\n            \n            # Value loss\n            value_loss = jnp.mean((values - returns) ** 2)\n            \n            return policy_loss + value_loss\n        \n        # Compute gradients and update\n        loss, grads = eqx.filter_value_and_grad(policy_loss_fn)(agent)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        agent = eqx.apply_updates(agent, updates)\n        \n        # Record history\n        history['episode_rewards'].append(float(episode_reward))\n        history['episode_lengths'].append(episode_length)\n        history['policy_losses'].append(float(loss))\n        \n        if verbose and episode % 100 == 0:\n            avg_reward = jnp.mean(jnp.array(history['episode_rewards'][-100:]))\n            logger.info(f\"Episode {episode}: avg_reward={avg_reward:.4f}, loss={loss:.6f}\")\n    \n    return agent, history\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    logging.basicConfig(level=logging.INFO)\n    \n    # Create environment\n    symbols = ['BTC/USDT', 'ETH/USDT', 'BNB/USDT']\n    env = TradingEnvironment(symbols)\n    \n    # Create agent\n    state_dim = len(symbols) + 20 + len(symbols) + 1  # prices + features + portfolio + cash\n    action_dim = len(symbols)\n    \n    agent = CryptoTradingAgent(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        key=jax.random.PRNGKey(42)\n    )\n    \n    # Quick test\n    state = env.reset()\n    state_array = jnp.concatenate([\n        state.prices,\n        state.features,\n        state.portfolio,\n        jnp.array([state.cash])\n    ])\n    \n    action = agent.get_action(state_array)\n    logger.info(f\"Generated action: positions={action.positions}, confidence={action.confidence}\")\n    \n    # Risk manager test\n    risk_manager = RiskManager()\n    is_safe, warnings, modified_action = risk_manager.check_risk_limits(\n        action, 100000.0, 0.5\n    )\n    \n    logger.info(f\"Risk check: safe={is_safe}, warnings={warnings}\")\n    logger.info(\"Crypto trading agent initialized successfully\")",
  "crypto_dp/data/__init__.py": "# Data ingestion and validation module",
  "crypto_dp/data/ingest.py": "\"\"\"\nData ingestion module for crypto market data.\n\nSupports multiple data sources:\n- CCXT for exchange data (public + private REST & websockets)\n- CoinGecko for market cap and price data\n- DuckDB for efficient storage and querying\n\"\"\"\n\nfrom typing import Optional, List, Dict, Any\nimport datetime as dt\nimport logging\n\nimport ccxt\nimport duckdb\nimport polars as pl\nfrom pycoingecko import CoinGeckoAPI\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef fetch_ohlcv(\n    symbol: str,\n    start: int,\n    end: int,\n    timeframe: str = \"1h\",\n    exchange: str = \"binance\"\n) -> pl.DataFrame:\n    \"\"\"\n    Fetch OHLCV data from a crypto exchange.\n    \n    Args:\n        symbol: Trading pair symbol (e.g., 'BTC/USDT')\n        start: Start timestamp in milliseconds\n        end: End timestamp in milliseconds\n        timeframe: Timeframe for candlesticks ('1m', '5m', '1h', '1d', etc.)\n        exchange: Exchange name (default: 'binance')\n    \n    Returns:\n        Polars DataFrame with OHLCV data\n    \"\"\"\n    try:\n        # Initialize exchange\n        exchange_class = getattr(ccxt, exchange)\n        ex = exchange_class({\n            'rateLimit': 1200,\n            'enableRateLimit': True,\n        })\n        \n        # Fetch data\n        data = ex.fetch_ohlcv(symbol, timeframe=timeframe, since=start, limit=None)\n        \n        # Convert to DataFrame\n        df = pl.DataFrame(\n            data,\n            schema=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n        )\n        \n        # Filter by end time\n        df = df.filter(pl.col(\"timestamp\") < end)\n        \n        # Add metadata columns\n        df = df.with_columns([\n            pl.lit(symbol).alias(\"symbol\"),\n            pl.lit(timeframe).alias(\"timeframe\"),\n            pl.lit(exchange).alias(\"exchange\"),\n            pl.from_epoch(pl.col(\"timestamp\") // 1000).alias(\"datetime\")\n        ])\n        \n        logger.info(f\"Fetched {len(df)} rows for {symbol} from {exchange}\")\n        return df\n        \n    except Exception as e:\n        logger.error(f\"Failed to fetch {symbol} from {exchange}: {e}\")\n        raise\n\n\ndef fetch_coingecko_data(\n    coin_ids: List[str],\n    vs_currency: str = \"usd\",\n    days: int = 365\n) -> pl.DataFrame:\n    \"\"\"\n    Fetch historical price data from CoinGecko.\n    \n    Args:\n        coin_ids: List of CoinGecko coin IDs\n        vs_currency: Currency to price against (default: 'usd')\n        days: Number of days of historical data\n    \n    Returns:\n        Polars DataFrame with price, market cap, and volume data\n    \"\"\"\n    cg = CoinGeckoAPI()\n    \n    all_data = []\n    for coin_id in coin_ids:\n        try:\n            data = cg.get_coin_market_chart_by_id(\n                id=coin_id,\n                vs_currency=vs_currency,\n                days=days\n            )\n            \n            # Extract time series data\n            prices = data['prices']\n            market_caps = data['market_caps']\n            volumes = data['total_volumes']\n            \n            # Create DataFrame\n            df = pl.DataFrame({\n                \"timestamp\": [p[0] for p in prices],\n                \"price\": [p[1] for p in prices],\n                \"market_cap\": [m[1] for m in market_caps],\n                \"volume\": [v[1] for v in volumes],\n                \"coin_id\": coin_id,\n                \"vs_currency\": vs_currency\n            })\n            \n            df = df.with_columns(\n                pl.from_epoch(pl.col(\"timestamp\") // 1000).alias(\"datetime\")\n            )\n            \n            all_data.append(df)\n            logger.info(f\"Fetched {len(df)} rows for {coin_id} from CoinGecko\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to fetch {coin_id} from CoinGecko: {e}\")\n            continue\n    \n    if all_data:\n        return pl.concat(all_data)\n    else:\n        return pl.DataFrame()\n\n\ndef load_to_duck(\n    db_path: str,\n    df: pl.DataFrame,\n    table: str,\n    mode: str = \"replace\"\n) -> None:\n    \"\"\"\n    Load DataFrame to DuckDB.\n    \n    Args:\n        db_path: Path to DuckDB database file\n        df: Polars DataFrame to load\n        table: Table name\n        mode: Load mode ('replace', 'append', 'upsert')\n    \"\"\"\n    con = None\n    try:\n        con = duckdb.connect(db_path)\n        \n        # Register the DataFrame with DuckDB so it can be referenced in SQL\n        con.register(\"temp_df\", df)\n        \n        if mode == \"replace\":\n            con.execute(f\"DROP TABLE IF EXISTS {table}\")\n            con.execute(f\"CREATE TABLE {table} AS SELECT * FROM temp_df\")\n        elif mode == \"append\":\n            con.execute(f\"INSERT INTO {table} SELECT * FROM temp_df\")\n        elif mode == \"upsert\":\n            # Simple upsert based on timestamp and symbol\n            con.execute(f\"\"\"\n                INSERT INTO {table} \n                SELECT * FROM temp_df \n                WHERE NOT EXISTS (\n                    SELECT 1 FROM {table} t2 \n                    WHERE t2.timestamp = temp_df.timestamp \n                    AND t2.symbol = temp_df.symbol\n                )\n            \"\"\")\n        \n        logger.info(f\"Loaded {len(df)} rows to {table} in {db_path}\")\n        \n    except Exception as e:\n        logger.error(f\"Failed to load data to {table}: {e}\")\n        raise\n    finally:\n        if con is not None:\n            con.close()\n\n\ndef get_top_crypto_symbols(limit: int = 50) -> List[str]:\n    \"\"\"\n    Get top cryptocurrency symbols by market cap.\n    \n    Args:\n        limit: Number of top coins to return\n    \n    Returns:\n        List of trading pair symbols (e.g., ['BTC/USDT', 'ETH/USDT', ...])\n    \"\"\"\n    try:\n        cg = CoinGeckoAPI()\n        coins = cg.get_coins_markets(\n            vs_currency='usd',\n            order='market_cap_desc',\n            per_page=limit,\n            page=1\n        )\n        \n        # Convert to exchange symbols (assuming USDT pairs)\n        symbols = []\n        for coin in coins:\n            symbol = coin['symbol'].upper()\n            if symbol not in ['USDT', 'USDC', 'BUSD']:  # Skip stablecoins\n                symbols.append(f\"{symbol}/USDT\")\n        \n        return symbols[:limit]\n        \n    except Exception as e:\n        logger.error(f\"Failed to get top crypto symbols: {e}\")\n        return []\n\n\ndef create_sample_dataset(\n    db_path: str = \"crypto_data.db\",\n    symbols: Optional[List[str]] = None,\n    days: int = 365\n) -> None:\n    \"\"\"\n    Create a sample crypto dataset for testing and development.\n    \n    Args:\n        db_path: Path to DuckDB database file\n        symbols: List of symbols to fetch (if None, uses top 10)\n        days: Number of days of historical data\n    \"\"\"\n    if symbols is None:\n        symbols = get_top_crypto_symbols(10)\n    \n    # Calculate time range\n    end_time = int(dt.datetime.now().timestamp() * 1000)\n    start_time = end_time - (days * 24 * 60 * 60 * 1000)\n    \n    logger.info(f\"Creating sample dataset with {len(symbols)} symbols\")\n    \n    for symbol in symbols:\n        try:\n            # Fetch OHLCV data\n            df = fetch_ohlcv(symbol, start_time, end_time, timeframe=\"1h\")\n            \n            if not df.is_empty():\n                load_to_duck(db_path, df, \"ohlcv\", mode=\"append\")\n                \n        except Exception as e:\n            logger.warning(f\"Skipped {symbol}: {e}\")\n            continue\n    \n    logger.info(f\"Sample dataset created at {db_path}\")\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    logging.basicConfig(level=logging.INFO)\n    create_sample_dataset()",
  "crypto_dp/phi/integration.py": "\"\"\"\n-layer integration: Bridging symbolic and neural components.\n\nThis module provides the integration point between the -layer and\nthe E2E-DP system, implementing the hybrid neuro-symbolic loss function.\n\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nfrom typing import Dict, Tuple, Optional, Any, Callable\nfrom dataclasses import dataclass\n\nfrom .layer import PhiLayer\n\n\n@dataclass\nclass PhiIntegrationConfig:\n    \"\"\"Configuration for -layer integration.\"\"\"\n    phi_weight: float = 1.0                    # Weight of  penalties in total loss\n    orthogonality_penalty: float = 0.01        # Penalty for DP- interference\n    curriculum_schedule: str = \"linear\"        # How to schedule  weight over time\n    min_phi_weight: float = 0.1               # Minimum  weight during curriculum\n    max_phi_weight: float = 2.0               # Maximum  weight during curriculum\n    decay_schedule: bool = True               # Whether to decay rule weights\n    gradient_monitoring: bool = True          # Whether to monitor gradient health\n\n\nclass PhiGuidedLoss(eqx.Module):\n    \"\"\"\n    -guided loss function that combines E2E-DP with symbolic knowledge.\n    \n    Implements the hybrid loss from CLAUDE.md Section 7.2:\n    L_total = L_dp +  w  soft_penalty_i()\n    \"\"\"\n    \n    phi_layer: PhiLayer\n    config: PhiIntegrationConfig\n    base_loss_fn: Callable\n    step_count: int\n    \n    def __init__(\n        self, \n        phi_layer: PhiLayer, \n        base_loss_fn: Callable,\n        config: Optional[PhiIntegrationConfig] = None\n    ):\n        \"\"\"\n        Initialize -guided loss.\n        \n        Args:\n            phi_layer: The -layer with symbolic rules\n            base_loss_fn: Base E2E-DP loss function (e.g., negative Sharpe)\n            config: Configuration for integration\n        \"\"\"\n        self.phi_layer = phi_layer\n        self.base_loss_fn = base_loss_fn\n        self.config = config or PhiIntegrationConfig()\n        self.step_count = 0\n    \n    def __call__(\n        self, \n        positions: jnp.ndarray, \n        state: Dict[str, jnp.ndarray],\n        returns: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, Dict[str, Any]]:\n        \"\"\"\n        Compute -guided loss.\n        \n        Args:\n            positions: Portfolio positions\n            state: Market state dictionary\n            returns: Portfolio returns for base loss\n            \n        Returns:\n            Total loss, diagnostic information\n        \"\"\"\n        # Compute base E2E-DP loss\n        base_loss = self.base_loss_fn(returns)\n        \n        # Compute -layer penalties\n        phi_penalty, phi_info = self.phi_layer(positions, state)\n        \n        # Apply curriculum scheduling\n        phi_weight = self._get_curriculum_weight()\n        \n        # Combine losses\n        total_loss = base_loss + phi_weight * phi_penalty\n        \n        # Add orthogonality penalty if enabled\n        if self.config.orthogonality_penalty > 0:\n            orthogonal_penalty = self._compute_dp_phi_orthogonality(\n                positions, state, returns\n            )\n            total_loss += self.config.orthogonality_penalty * orthogonal_penalty\n        else:\n            orthogonal_penalty = 0.0\n        \n        # Prepare diagnostic info\n        diagnostics = {\n            'base_loss': float(base_loss),\n            'phi_penalty': float(phi_penalty),\n            'phi_weight': float(phi_weight),\n            'total_loss': float(total_loss),\n            'orthogonal_penalty': float(orthogonal_penalty),\n            'step_count': self.step_count,\n            'phi_info': phi_info,\n            'loss_breakdown': {\n                'base_pct': float(base_loss / total_loss) * 100,\n                'phi_pct': float(phi_weight * phi_penalty / total_loss) * 100,\n                'ortho_pct': float(self.config.orthogonality_penalty * orthogonal_penalty / total_loss) * 100\n            }\n        }\n        \n        return total_loss, diagnostics\n    \n    def _get_curriculum_weight(self) -> float:\n        \"\"\"\n        Get current  weight based on curriculum schedule.\n        \n        Returns:\n            Current  weight\n        \"\"\"\n        if self.config.curriculum_schedule == \"constant\":\n            return self.config.phi_weight\n        \n        elif self.config.curriculum_schedule == \"linear\":\n            # Linear increase from min to max over first 1000 steps\n            progress = min(self.step_count / 1000.0, 1.0)\n            return (\n                self.config.min_phi_weight + \n                progress * (self.config.max_phi_weight - self.config.min_phi_weight)\n            )\n        \n        elif self.config.curriculum_schedule == \"exponential\":\n            # Exponential increase\n            alpha = 0.001\n            weight = self.config.min_phi_weight * jnp.exp(alpha * self.step_count)\n            return jnp.clip(weight, self.config.min_phi_weight, self.config.max_phi_weight)\n        \n        else:\n            return self.config.phi_weight\n    \n    def _compute_dp_phi_orthogonality(\n        self, \n        positions: jnp.ndarray, \n        state: Dict[str, jnp.ndarray],\n        returns: jnp.ndarray\n    ) -> jnp.ndarray:\n        \"\"\"\n        Compute orthogonality penalty between DP and  gradients.\n        \n        This prevents double-counting when both systems learn the same effect.\n        \"\"\"\n        # Gradient of base loss w.r.t. positions\n        grad_base = jax.grad(lambda pos: self.base_loss_fn(returns))(positions)\n        \n        # Gradient of  penalty w.r.t. positions  \n        grad_phi = jax.grad(lambda pos: self.phi_layer(pos, state)[0])(positions)\n        \n        # Dot product measures alignment (positive = same direction)\n        dot_product = jnp.dot(grad_base, grad_phi)\n        \n        # Penalty for excessive alignment\n        return dot_product ** 2\n    \n    def step(self) -> 'PhiGuidedLoss':\n        \"\"\"\n        Advance one training step (for curriculum scheduling).\n        \n        Returns:\n            Updated PhiGuidedLoss with incremented step count\n        \"\"\"\n        new_step_count = self.step_count + 1\n        updated_loss = eqx.tree_at(\n            lambda loss: loss.step_count,\n            self,\n            new_step_count\n        )\n        \n        # Apply rule weight decay if enabled\n        if self.config.decay_schedule:\n            decayed_phi_layer = self.phi_layer.decay_weights()\n            updated_loss = eqx.tree_at(\n                lambda loss: loss.phi_layer,\n                updated_loss,\n                decayed_phi_layer\n            )\n        \n        return updated_loss\n    \n    def update_phi_weights(\n        self, \n        performance_metrics: Dict[str, float],\n        learning_rate: float = 0.01\n    ) -> 'PhiGuidedLoss':\n        \"\"\"\n        Update -layer rule weights based on performance.\n        \n        Args:\n            performance_metrics: Dictionary of rule_name -> performance\n            learning_rate: Learning rate for weight updates\n            \n        Returns:\n            Updated PhiGuidedLoss with new rule weights\n        \"\"\"\n        updated_phi_layer = self.phi_layer.update_attention(\n            performance_metrics, learning_rate\n        )\n        \n        return eqx.tree_at(\n            lambda loss: loss.phi_layer,\n            self,\n            updated_phi_layer\n        )\n    \n    def get_explanation(\n        self, \n        positions: jnp.ndarray, \n        state: Dict[str, jnp.ndarray]\n    ) -> str:\n        \"\"\"\n        Generate explanation of loss components.\n        \n        Args:\n            positions: Current positions\n            state: Market state\n            \n        Returns:\n            Human-readable explanation\n        \"\"\"\n        phi_explanation = self.phi_layer.explain_decision(positions, state)\n        phi_weight = self._get_curriculum_weight()\n        \n        return f\"\"\"\n-Guided Loss Analysis (Step {self.step_count}):\n-  weight: {phi_weight:.3f}\n- Curriculum schedule: {self.config.curriculum_schedule}\n\n{phi_explanation}\n\nIntegration status:\n- Orthogonality penalty: {self.config.orthogonality_penalty:.3f}\n- Decay schedule: {self.config.decay_schedule}\n\"\"\"\n\n\n# Factory functions for common configurations\ndef create_minimal_phi_guided_loss(\n    base_loss_fn: Callable,\n    key: Optional[jax.random.PRNGKey] = None\n) -> PhiGuidedLoss:\n    \"\"\"\n    Create minimal -guided loss with single volatility rule.\n    \n    Implements the minimal POC from CLAUDE.md Section 7.5.\n    \"\"\"\n    from .rules import VolatilityRule\n    from .layer import PhiLayer\n    \n    # Single volatility rule\n    rules = {'volatility': VolatilityRule(vol_threshold=2.0, initial_weight=1.0)}\n    phi_layer = PhiLayer(rules, key=key)\n    \n    # Conservative integration config\n    config = PhiIntegrationConfig(\n        phi_weight=0.5,\n        orthogonality_penalty=0.01,\n        curriculum_schedule=\"linear\",\n        min_phi_weight=0.1,\n        max_phi_weight=1.0\n    )\n    \n    return PhiGuidedLoss(phi_layer, base_loss_fn, config)\n\n\ndef create_full_phi_guided_loss(\n    base_loss_fn: Callable,\n    key: Optional[jax.random.PRNGKey] = None\n) -> PhiGuidedLoss:\n    \"\"\"\n    Create full -guided loss with multiple rules.\n    \n    Implements the complete hybrid system from CLAUDE.md Section 7.6.\n    \"\"\"\n    from .layer import create_default_phi_layer\n    \n    phi_layer = create_default_phi_layer(key)\n    \n    # Full integration config\n    config = PhiIntegrationConfig(\n        phi_weight=1.0,\n        orthogonality_penalty=0.01,\n        curriculum_schedule=\"exponential\",\n        min_phi_weight=0.1,\n        max_phi_weight=2.0,\n        decay_schedule=True,\n        gradient_monitoring=True\n    )\n    \n    return PhiGuidedLoss(phi_layer, base_loss_fn, config)\n\n\n# Utility functions\ndef phi_sharpe_loss(returns: jnp.ndarray, epsilon: float = 1e-6) -> jnp.ndarray:\n    \"\"\"\n    Smooth Sharpe ratio loss compatible with -guided loss.\n    \n    This is the base loss function from basic_e2e.py, adapted for  integration.\n    \"\"\"\n    mean_return = jnp.mean(returns)\n    std_return = jnp.sqrt(jnp.var(returns) + epsilon)\n    sharpe = mean_return / std_return\n    return -sharpe  # Negative for minimization",
  "crypto_dp/phi/rules.py": "\"\"\"\n-layer rules: Symbolic knowledge encoded as differentiable penalties.\n\nEach rule represents a piece of domain knowledge that can be:\n1. Evaluated as a soft constraint penalty\n2. Updated based on gradient feedback\n3. Interpreted for explanation generation\n\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nfrom typing import Dict, Any, Tuple, Optional\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass PhiRuleMetrics:\n    \"\"\"Metrics for monitoring rule activation and effectiveness.\"\"\"\n    activation_frequency: float\n    penalty_magnitude: float\n    gradient_magnitude: float\n    concept_drift: float\n    \n    def is_healthy(self) -> Tuple[bool, list]:\n        \"\"\"Check if rule is functioning properly.\"\"\"\n        issues = []\n        \n        if self.activation_frequency < 0.01:\n            issues.append(\"Rule rarely activates\")\n        elif self.activation_frequency > 0.95:\n            issues.append(\"Rule always activates\")\n            \n        if self.penalty_magnitude < 1e-6:\n            issues.append(\"Penalty too weak\")\n        elif self.penalty_magnitude > 100:\n            issues.append(\"Penalty too strong\")\n            \n        if self.gradient_magnitude < 1e-8:\n            issues.append(\"No learning signal\")\n            \n        return len(issues) == 0, issues\n\n\nclass PhiRule(eqx.Module, ABC):\n    \"\"\"\n    Abstract base class for -layer rules.\n    \n    A rule combines:\n    - Symbolic concept (e.g., \"high volatility\")\n    - Differentiable trigger function (smooth threshold)\n    - Penalty function (shapes loss landscape)\n    - Learnable weight (importance/strength)\n    \"\"\"\n    \n    weight: jnp.ndarray\n    name: str\n    \n    def __init__(self, name: str, initial_weight: float = 1.0):\n        self.name = name\n        self.weight = jnp.array(initial_weight)\n    \n    @abstractmethod\n    def trigger(self, state: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n        \"\"\"\n        Evaluate rule activation based on market state.\n        \n        Args:\n            state: Market state dictionary\n            \n        Returns:\n            Activation strength [0, 1] (differentiable)\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def penalty(self, positions: jnp.ndarray, state: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n        \"\"\"\n        Compute penalty for current positions given state.\n        \n        Args:\n            positions: Current portfolio positions\n            state: Market state dictionary\n            \n        Returns:\n            Penalty value (positive = violation)\n        \"\"\"\n        pass\n    \n    def apply(self, positions: jnp.ndarray, state: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n        \"\"\"\n        Apply rule to current state and positions.\n        \n        Returns:\n            Weighted penalty contribution to loss\n        \"\"\"\n        activation = self.trigger(state)\n        base_penalty = self.penalty(positions, state)\n        \n        # Smooth gating: only apply penalty when rule is triggered\n        return self.weight * activation * base_penalty\n    \n    def get_explanation(self, state: Dict[str, jnp.ndarray]) -> str:\n        \"\"\"Generate human-readable explanation of rule activation.\"\"\"\n        activation = float(self.trigger(state))\n        return f\"{self.name}: {activation:.1%} activated (weight: {float(self.weight):.3f})\"\n\n\nclass VolatilityRule(PhiRule):\n    \"\"\"\n    Rule: \"Reduce position size in high volatility regimes\"\n    \n    Implements the minimal -concept from CLAUDE.md Section 7.5.\n    \"\"\"\n    \n    vol_threshold: float\n    steepness: float\n    \n    def __init__(\n        self, \n        vol_threshold: float = 2.0, \n        steepness: float = 10.0,\n        initial_weight: float = 1.0\n    ):\n        super().__init__(\"VolatilityRule\", initial_weight)\n        self.vol_threshold = vol_threshold\n        self.steepness = steepness\n    \n    def trigger(self, state: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n        \"\"\"\n        Smooth trigger based on volatility level.\n        \n        Args:\n            state: Must contain 'volatility' key with market volatility\n            \n        Returns:\n            Sigmoid activation based on volatility threshold\n        \"\"\"\n        volatility = state.get('volatility', 0.0)\n        # Smooth sigmoid trigger (replaces hard threshold)\n        return jax.nn.sigmoid(self.steepness * (volatility - self.vol_threshold))\n    \n    def penalty(self, positions: jnp.ndarray, state: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n        \"\"\"\n        Penalty proportional to position size squared.\n        \n        Encourages position reduction in high volatility.\n        \"\"\"\n        risk_budget = state.get('risk_budget', 1.0)\n        \n        # L2 penalty on positions, normalized by risk budget\n        return jnp.sum(positions ** 2) / risk_budget\n    \n    def get_explanation(self, state: Dict[str, jnp.ndarray]) -> str:\n        \"\"\"Explain volatility rule activation.\"\"\"\n        volatility = float(state.get('volatility', 0.0))\n        activation = float(self.trigger(state))\n        \n        return (\n            f\"VolatilityRule: Market vol={volatility:.2f} (threshold={self.vol_threshold:.2f}), \"\n            f\"activation={activation:.1%}, suggests {'reducing' if activation > 0.5 else 'maintaining'} positions\"\n        )\n\n\nclass RiskBudgetRule(PhiRule):\n    \"\"\"\n    Rule: \"Respect maximum risk budget allocation\"\n    \n    Implements position limit constraints as soft penalties.\n    \"\"\"\n    \n    max_position_pct: float\n    \n    def __init__(self, max_position_pct: float = 0.2, initial_weight: float = 2.0):\n        super().__init__(\"RiskBudgetRule\", initial_weight)\n        self.max_position_pct = max_position_pct\n    \n    def trigger(self, state: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n        \"\"\"Always active - risk limits always apply.\"\"\"\n        return jnp.array(1.0)\n    \n    def penalty(self, positions: jnp.ndarray, state: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n        \"\"\"\n        Penalty for exceeding position limits.\n        \n        Uses smooth hinge loss for differentiability.\n        \"\"\"\n        # Soft hinge loss for position limits\n        excess = jnp.maximum(0, jnp.abs(positions) - self.max_position_pct)\n        return jnp.sum(excess ** 2)\n    \n    def get_explanation(self, state: Dict[str, jnp.ndarray]) -> str:\n        \"\"\"Explain risk budget rule.\"\"\"\n        return (\n            f\"RiskBudgetRule: Max position {self.max_position_pct:.1%} per asset, \"\n            f\"enforces portfolio concentration limits\"\n        )\n\n\nclass MomentumRule(PhiRule):\n    \"\"\"\n    Rule: \"Follow momentum in trending markets\"\n    \n    Example of a more complex rule that considers market regime.\n    \"\"\"\n    \n    momentum_threshold: float\n    trend_strength: float\n    \n    def __init__(\n        self, \n        momentum_threshold: float = 0.05, \n        trend_strength: float = 5.0,\n        initial_weight: float = 0.5\n    ):\n        super().__init__(\"MomentumRule\", initial_weight)\n        self.momentum_threshold = momentum_threshold\n        self.trend_strength = trend_strength\n    \n    def trigger(self, state: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n        \"\"\"\n        Trigger based on momentum strength.\n        \"\"\"\n        momentum = state.get('momentum', 0.0)\n        return jax.nn.sigmoid(self.trend_strength * (jnp.abs(momentum) - self.momentum_threshold))\n    \n    def penalty(self, positions: jnp.ndarray, state: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n        \"\"\"\n        Penalty for going against momentum.\n        \n        Encourages alignment with trend direction.\n        \"\"\"\n        momentum = state.get('momentum', 0.0)\n        expected_returns = state.get('expected_returns', jnp.zeros_like(positions))\n        \n        # Penalty for misalignment between positions and expected returns\n        alignment = jnp.sum(positions * expected_returns)\n        return jnp.maximum(0, -alignment)  # Penalty when negative alignment\n    \n    def get_explanation(self, state: Dict[str, jnp.ndarray]) -> str:\n        \"\"\"Explain momentum rule.\"\"\"\n        momentum = float(state.get('momentum', 0.0))\n        activation = float(self.trigger(state))\n        \n        direction = \"bullish\" if momentum > 0 else \"bearish\"\n        return (\n            f\"MomentumRule: Market momentum={momentum:.3f} ({direction}), \"\n            f\"activation={activation:.1%}, suggests {'following' if activation > 0.5 else 'ignoring'} trend\"\n        )\n\n\n# Factory function for creating common rule sets\ndef create_basic_rule_set() -> Dict[str, PhiRule]:\n    \"\"\"Create a basic set of trading rules.\"\"\"\n    return {\n        'volatility': VolatilityRule(vol_threshold=2.0, initial_weight=1.0),\n        'risk_budget': RiskBudgetRule(max_position_pct=0.2, initial_weight=2.0),\n        'momentum': MomentumRule(momentum_threshold=0.05, initial_weight=0.5)\n    }\n\n\ndef create_conservative_rule_set() -> Dict[str, PhiRule]:\n    \"\"\"Create a conservative trading rule set.\"\"\"\n    return {\n        'volatility': VolatilityRule(vol_threshold=1.5, initial_weight=2.0),  # More sensitive\n        'risk_budget': RiskBudgetRule(max_position_pct=0.1, initial_weight=3.0),  # Stricter limits\n    }\n\n\ndef create_aggressive_rule_set() -> Dict[str, PhiRule]:\n    \"\"\"Create an aggressive trading rule set.\"\"\"\n    return {\n        'volatility': VolatilityRule(vol_threshold=3.0, initial_weight=0.5),  # Less sensitive\n        'risk_budget': RiskBudgetRule(max_position_pct=0.3, initial_weight=1.0),  # Looser limits\n        'momentum': MomentumRule(momentum_threshold=0.02, initial_weight=1.5)  # More momentum focus\n    }",
  "crypto_dp/phi/layer.py": "\"\"\"\n-layer: Collection of rules with attention-based activation.\n\nThe PhiLayer aggregates multiple rules and provides:\n1. Attention-weighted rule combination\n2. Rule decay and meta-learning\n3. Gradient attribution for interpretability\n\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nfrom typing import Dict, List, Tuple, Optional, Any\nimport time\nfrom dataclasses import dataclass\n\nfrom .rules import PhiRule, PhiRuleMetrics\n\n\n@dataclass\nclass PhiLayerMetrics:\n    \"\"\"Metrics for monitoring the entire -layer.\"\"\"\n    rule_metrics: Dict[str, PhiRuleMetrics]\n    attention_weights: Dict[str, float]\n    total_penalty: float\n    active_rules: List[str]\n    gradient_health: Dict[str, float]\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics.\"\"\"\n        return {\n            'num_active_rules': len(self.active_rules),\n            'total_penalty': self.total_penalty,\n            'max_attention': max(self.attention_weights.values()) if self.attention_weights else 0.0,\n            'healthy_rules': sum(1 for rule_metrics in self.rule_metrics.values() \n                               if rule_metrics.is_healthy()[0])\n        }\n\n\nclass PhiLayer(eqx.Module):\n    \"\"\"\n    -layer: Neuro-symbolic knowledge integration layer.\n    \n    Combines multiple symbolic rules with learnable attention weights\n    and provides differentiable penalties for loss shaping.\n    \"\"\"\n    \n    rules: Dict[str, PhiRule]\n    attention_weights: jnp.ndarray\n    rule_names: List[str]\n    decay_rate: float\n    orthogonality_penalty: float\n    \n    def __init__(\n        self, \n        rules: Dict[str, PhiRule], \n        decay_rate: float = 0.99,\n        orthogonality_penalty: float = 0.01,\n        key: Optional[jax.random.PRNGKey] = None\n    ):\n        \"\"\"\n        Initialize -layer.\n        \n        Args:\n            rules: Dictionary of rule_name -> PhiRule\n            decay_rate: Decay rate for rule weights over time\n            orthogonality_penalty: Penalty for rule interference\n            key: Random key for initialization\n        \"\"\"\n        if key is None:\n            key = jax.random.PRNGKey(42)\n        \n        self.rules = rules\n        self.rule_names = list(rules.keys())\n        self.decay_rate = decay_rate\n        self.orthogonality_penalty = orthogonality_penalty\n        \n        # Initialize attention weights\n        n_rules = len(rules)\n        self.attention_weights = jax.nn.softmax(\n            jax.random.normal(key, (n_rules,)) * 0.1\n        )\n    \n    def __call__(\n        self, \n        positions: jnp.ndarray, \n        state: Dict[str, jnp.ndarray]\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n        \"\"\"\n        Apply -layer to compute penalty and rule activations.\n        \n        Args:\n            positions: Current portfolio positions\n            state: Market state dictionary\n            \n        Returns:\n            Total penalty, individual rule penalties\n        \"\"\"\n        rule_penalties = {}\n        rule_activations = {}\n        \n        # Compute individual rule penalties\n        for i, (rule_name, rule) in enumerate(self.rules.items()):\n            penalty = rule.apply(positions, state)\n            activation = rule.trigger(state)\n            \n            rule_penalties[rule_name] = penalty\n            rule_activations[rule_name] = activation\n        \n        # Attention-weighted combination\n        total_penalty = 0.0\n        for i, rule_name in enumerate(self.rule_names):\n            attention_weight = self.attention_weights[i]\n            total_penalty += attention_weight * rule_penalties[rule_name]\n        \n        # Add orthogonality penalty to prevent rule interference\n        if self.orthogonality_penalty > 0:\n            total_penalty += self._compute_orthogonality_penalty(\n                positions, state, rule_penalties\n            )\n        \n        return total_penalty, {\n            'penalties': rule_penalties,\n            'activations': rule_activations,\n            'attention_weights': dict(zip(self.rule_names, self.attention_weights))\n        }\n    \n    def _compute_orthogonality_penalty(\n        self, \n        positions: jnp.ndarray, \n        state: Dict[str, jnp.ndarray],\n        rule_penalties: Dict[str, jnp.ndarray]\n    ) -> jnp.ndarray:\n        \"\"\"\n        Compute orthogonality penalty to prevent rule interference.\n        \n        This prevents rules from having the same effect (double-counting).\n        \"\"\"\n        # Get gradients of each rule penalty w.r.t. positions\n        rule_gradients = {}\n        for rule_name, rule in self.rules.items():\n            grad_fn = jax.grad(lambda pos: rule.apply(pos, state))\n            rule_gradients[rule_name] = grad_fn(positions)\n        \n        # Compute dot products between gradients\n        penalty = 0.0\n        rule_names = list(rule_gradients.keys())\n        \n        for i in range(len(rule_names)):\n            for j in range(i + 1, len(rule_names)):\n                grad_i = rule_gradients[rule_names[i]]\n                grad_j = rule_gradients[rule_names[j]]\n                \n                # Penalty for parallel gradients (same effect)\n                dot_product = jnp.dot(grad_i, grad_j)\n                penalty += dot_product ** 2\n        \n        return self.orthogonality_penalty * penalty\n    \n    def update_attention(\n        self, \n        rule_performance: Dict[str, float],\n        learning_rate: float = 0.01\n    ) -> 'PhiLayer':\n        \"\"\"\n        Update attention weights based on rule performance.\n        \n        Args:\n            rule_performance: Dictionary of rule_name -> performance metric\n            learning_rate: Learning rate for attention updates\n            \n        Returns:\n            Updated PhiLayer\n        \"\"\"\n        # Convert performance to gradients\n        performance_array = jnp.array([\n            rule_performance.get(name, 0.0) \n            for name in self.rule_names\n        ])\n        \n        # Softmax gradient update\n        attention_logits = jnp.log(self.attention_weights + 1e-8)\n        updated_logits = attention_logits + learning_rate * performance_array\n        updated_weights = jax.nn.softmax(updated_logits)\n        \n        # Create new layer with updated weights\n        new_layer = eqx.tree_at(\n            lambda layer: layer.attention_weights,\n            self,\n            updated_weights\n        )\n        \n        return new_layer\n    \n    def decay_weights(self) -> 'PhiLayer':\n        \"\"\"\n        Apply decay to rule weights to prevent stale concepts.\n        \n        Returns:\n            PhiLayer with decayed rule weights\n        \"\"\"\n        decayed_rules = {}\n        \n        for rule_name, rule in self.rules.items():\n            decayed_weight = rule.weight * self.decay_rate\n            decayed_rule = eqx.tree_at(\n                lambda r: r.weight,\n                rule,\n                decayed_weight\n            )\n            decayed_rules[rule_name] = decayed_rule\n        \n        return eqx.tree_at(\n            lambda layer: layer.rules,\n            self,\n            decayed_rules\n        )\n    \n    def get_active_rules(\n        self, \n        state: Dict[str, jnp.ndarray], \n        threshold: float = 0.1\n    ) -> List[str]:\n        \"\"\"\n        Get list of currently active rules.\n        \n        Args:\n            state: Current market state\n            threshold: Activation threshold\n            \n        Returns:\n            List of active rule names\n        \"\"\"\n        active_rules = []\n        \n        for rule_name, rule in self.rules.items():\n            activation = rule.trigger(state)\n            if float(activation) > threshold:\n                active_rules.append(rule_name)\n        \n        return active_rules\n    \n    def explain_decision(\n        self, \n        positions: jnp.ndarray, \n        state: Dict[str, jnp.ndarray]\n    ) -> str:\n        \"\"\"\n        Generate human-readable explanation of -layer decision.\n        \n        Args:\n            positions: Current positions\n            state: Market state\n            \n        Returns:\n            Explanation string\n        \"\"\"\n        _, rule_info = self(positions, state)\n        \n        explanations = []\n        for i, rule_name in enumerate(self.rule_names):\n            rule = self.rules[rule_name]\n            attention = self.attention_weights[i]\n            activation = rule_info['activations'][rule_name]\n            \n            if float(activation) > 0.1:  # Only explain active rules\n                rule_explanation = rule.get_explanation(state)\n                explanations.append(\n                    f\"  [{attention:.1%} attention] {rule_explanation}\"\n                )\n        \n        if explanations:\n            return \"-layer active rules:\\n\" + \"\\n\".join(explanations)\n        else:\n            return \"-layer: No rules currently active\"\n    \n    def compute_metrics(\n        self, \n        positions: jnp.ndarray, \n        state: Dict[str, jnp.ndarray]\n    ) -> PhiLayerMetrics:\n        \"\"\"\n        Compute comprehensive metrics for monitoring.\n        \n        Args:\n            positions: Current positions\n            state: Market state\n            \n        Returns:\n            PhiLayerMetrics object\n        \"\"\"\n        total_penalty, rule_info = self(positions, state)\n        \n        # Compute rule-level metrics\n        rule_metrics = {}\n        for rule_name, rule in self.rules.items():\n            activation = rule_info['activations'][rule_name]\n            penalty = rule_info['penalties'][rule_name]\n            \n            # Compute gradient magnitude for the rule\n            grad_fn = jax.grad(lambda pos: rule.apply(pos, state))\n            gradient = grad_fn(positions)\n            grad_magnitude = float(jnp.linalg.norm(gradient))\n            \n            rule_metrics[rule_name] = PhiRuleMetrics(\n                activation_frequency=float(activation),\n                penalty_magnitude=float(penalty),\n                gradient_magnitude=grad_magnitude,\n                concept_drift=0.0  # TODO: Track over time\n            )\n        \n        return PhiLayerMetrics(\n            rule_metrics=rule_metrics,\n            attention_weights=rule_info['attention_weights'],\n            total_penalty=float(total_penalty),\n            active_rules=self.get_active_rules(state),\n            gradient_health={}  # TODO: Add gradient health metrics\n        )\n\n\n# Helper functions for common operations\ndef create_default_phi_layer(key: Optional[jax.random.PRNGKey] = None) -> PhiLayer:\n    \"\"\"Create a default -layer with basic trading rules.\"\"\"\n    from .rules import create_basic_rule_set\n    \n    rules = create_basic_rule_set()\n    return PhiLayer(rules, key=key)\n\n\ndef create_conservative_phi_layer(key: Optional[jax.random.PRNGKey] = None) -> PhiLayer:\n    \"\"\"Create a conservative -layer with risk-focused rules.\"\"\"\n    from .rules import create_conservative_rule_set\n    \n    rules = create_conservative_rule_set()\n    return PhiLayer(rules, decay_rate=0.95, key=key)  # Faster decay for conservative approach",
  "crypto_dp/phi/__init__.py": "\"\"\"\n-layer (Phi-layer): Neuro-symbolic knowledge integration for E2E-DP.\n\nThis module implements the hybrid neuro-symbolic architecture described in CLAUDE.md,\ncombining end-to-end differentiable programming with explicit symbolic knowledge.\n\nThe -layer provides:\n1. Symbolic concept representation (rules, constraints, domain knowledge)\n2. Differentiable penalty functions that shape the loss landscape\n3. Bidirectional updates between symbolic and neural components\n4. Interpretable explanations for learned behaviors\n\nArchitecture:\n- PhiRule: Individual symbolic rules with differentiable penalties\n- PhiLayer: Collection of rules with attention-based activation\n- PhiGuidedLoss: Integration point with E2E-DP loss functions\n\"\"\"\n\nfrom .rules import PhiRule, VolatilityRule, RiskBudgetRule\nfrom .layer import PhiLayer\nfrom .integration import PhiGuidedLoss\n\n__all__ = [\n    'PhiRule',\n    'VolatilityRule', \n    'RiskBudgetRule',\n    'PhiLayer',\n    'PhiGuidedLoss'\n]",
  "crypto_dp/pipelines/__init__.py": "\"\"\"E2E-DP pipeline implementations.\"\"\"\n\nfrom .basic_e2e import (\n    EndToEndDPPipeline,\n    MarketState,\n    TrainingConfig,\n    train_e2e_pipeline\n)\n\n__all__ = [\n    'EndToEndDPPipeline',\n    'MarketState', \n    'TrainingConfig',\n    'train_e2e_pipeline'\n]",
  "crypto_dp/pipelines/basic_e2e.py": "\"\"\"\nBasic end-to-end differentiable pipeline demonstration.\n\nThis module implements a simple E2E-DP system that flows gradients through:\n1. Feature extraction\n2. Prediction\n3. Decision making (portfolio optimization)\n4. Simulated returns\n5. Loss computation\n\nBased on CLAUDE.md architecture principles.\n\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom typing import Tuple, Dict, Any, NamedTuple\nimport equinox as eqx\nimport optax\nfrom dataclasses import dataclass\n\nfrom ..monitoring.gradient_health import EnhancedGradientMonitor, apply_global_gradient_clip\n\n\nclass MarketState(NamedTuple):\n    \"\"\"Simple market state representation.\"\"\"\n    prices: jnp.ndarray  # [n_assets]\n    volumes: jnp.ndarray  # [n_assets]\n    volatilities: jnp.ndarray  # [n_assets]\n    time_features: jnp.ndarray  # [n_time_features]\n\n\nclass E2EDPModule(eqx.Module):\n    \"\"\"Base class for differentiable modules in the pipeline.\"\"\"\n    \n    def check_gradients(self, grads: Any, monitor: EnhancedGradientMonitor) -> Dict[str, float]:\n        \"\"\"Check gradient health for this module.\"\"\"\n        metrics = monitor.compute_metrics(grads, prefix=self.__class__.__name__)\n        return {\n            'norm_ratio': metrics.norm_ratio,\n            'stv': metrics.signal_to_total_variance,\n            'sparsity': metrics.gradient_sparsity\n        }\n\n\nclass DifferentiableFeatureExtractor(E2EDPModule):\n    \"\"\"Learnable feature extraction from market data.\"\"\"\n    \n    price_encoder: eqx.nn.MLP\n    volume_encoder: eqx.nn.MLP\n    time_encoder: eqx.nn.MLP\n    fusion_layer: eqx.nn.Linear\n    \n    def __init__(self, n_assets: int, feature_dim: int = 32, key=None):\n        if key is None:\n            key = jax.random.PRNGKey(42)\n        \n        keys = jax.random.split(key, 4)\n        \n        # Separate encoders for different data modalities\n        self.price_encoder = eqx.nn.MLP(\n            in_size=n_assets,\n            out_size=feature_dim,\n            width_size=16,\n            depth=2,\n            key=keys[0]\n        )\n        \n        self.volume_encoder = eqx.nn.MLP(\n            in_size=n_assets,\n            out_size=feature_dim,\n            width_size=16,\n            depth=2,\n            key=keys[1]\n        )\n        \n        self.time_encoder = eqx.nn.MLP(\n            in_size=4,  # hour, day, week, month features\n            out_size=feature_dim // 2,\n            width_size=8,\n            depth=1,\n            key=keys[2]\n        )\n        \n        # Fusion layer\n        self.fusion_layer = eqx.nn.Linear(\n            in_features=feature_dim * 2 + feature_dim // 2,\n            out_features=feature_dim * 2,\n            key=keys[3]\n        )\n    \n    def __call__(self, market_state: MarketState) -> jnp.ndarray:\n        \"\"\"Extract features from market state.\"\"\"\n        # Encode each modality\n        price_features = self.price_encoder(market_state.prices)\n        volume_features = self.volume_encoder(jnp.log1p(market_state.volumes))\n        time_features = self.time_encoder(market_state.time_features)\n        \n        # Concatenate and fuse\n        combined = jnp.concatenate([price_features, volume_features, time_features])\n        return jax.nn.relu(self.fusion_layer(combined))\n\n\nclass DifferentiablePredictor(E2EDPModule):\n    \"\"\"Predict expected returns and risks.\"\"\"\n    \n    return_predictor: eqx.nn.MLP\n    risk_predictor: eqx.nn.MLP\n    \n    def __init__(self, feature_dim: int, n_assets: int, key=None):\n        if key is None:\n            key = jax.random.PRNGKey(43)\n        \n        keys = jax.random.split(key, 2)\n        \n        self.return_predictor = eqx.nn.MLP(\n            in_size=feature_dim,\n            out_size=n_assets,\n            width_size=32,\n            depth=2,\n            key=keys[0]\n        )\n        \n        self.risk_predictor = eqx.nn.MLP(\n            in_size=feature_dim,\n            out_size=n_assets,\n            width_size=32,\n            depth=2,\n            activation=jax.nn.softplus,  # Ensure positive risks\n            key=keys[1]\n        )\n    \n    def __call__(self, features: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        \"\"\"Predict returns and risks from features.\"\"\"\n        expected_returns = self.return_predictor(features)\n        predicted_risks = self.risk_predictor(features) + 1e-4  # Ensure non-zero\n        return expected_returns, predicted_risks\n\n\nclass DifferentiableDecisionMaker(E2EDPModule):\n    \"\"\"Make portfolio decisions based on predictions.\"\"\"\n    \n    temperature: float = 1.0\n    max_position: float = 0.2\n    \n    def __call__(\n        self,\n        expected_returns: jnp.ndarray,\n        predicted_risks: jnp.ndarray\n    ) -> jnp.ndarray:\n        \"\"\"\n        Generate portfolio weights using differentiable optimization.\n        \n        Simple mean-variance optimization with softmax relaxation.\n        \"\"\"\n        # Risk-adjusted scores\n        scores = expected_returns / (predicted_risks + 1e-8)\n        \n        # Temperature-scaled softmax for differentiable selection\n        scaled_scores = scores / self.temperature\n        weights = jax.nn.softmax(scaled_scores)\n        \n        # Soft position limits using tanh\n        weights = self.max_position * jnp.tanh(weights / self.max_position)\n        \n        # Renormalize\n        weights = weights / (jnp.sum(weights) + 1e-8)\n        \n        return weights\n\n\nclass DifferentiableSimulator(E2EDPModule):\n    \"\"\"Simulate market response and returns.\"\"\"\n    \n    market_impact: float = 0.001\n    volatility_scaling: float = 0.1\n    \n    def __call__(\n        self,\n        weights: jnp.ndarray,\n        market_state: MarketState,\n        key: jax.random.PRNGKey\n    ) -> jnp.ndarray:\n        \"\"\"\n        Simulate returns with market impact and noise.\n        \n        This is a simplified simulator - real version would use JAX-LOB.\n        \"\"\"\n        # Base returns (could be learned or historical)\n        base_returns = 0.0001 * jax.random.normal(key, weights.shape)\n        \n        # Market impact (penalty for large positions)\n        impact = -self.market_impact * weights**2\n        \n        # Volatility adjustment\n        vol_adjustment = market_state.volatilities * self.volatility_scaling\n        noise = vol_adjustment * jax.random.normal(jax.random.split(key)[0], weights.shape)\n        \n        # Total returns\n        returns = base_returns + impact + noise\n        \n        # Portfolio return\n        portfolio_return = jnp.dot(weights, returns)\n        \n        return portfolio_return\n\n\nclass EndToEndDPPipeline(E2EDPModule):\n    \"\"\"Complete E2E-DP pipeline from data to returns.\"\"\"\n    \n    feature_extractor: DifferentiableFeatureExtractor\n    predictor: DifferentiablePredictor\n    decision_maker: DifferentiableDecisionMaker\n    simulator: DifferentiableSimulator\n    \n    def __init__(self, n_assets: int = 10, feature_dim: int = 64, key=None):\n        if key is None:\n            key = jax.random.PRNGKey(100)\n        \n        keys = jax.random.split(key, 4)\n        \n        self.feature_extractor = DifferentiableFeatureExtractor(\n            n_assets, feature_dim, keys[0]\n        )\n        self.predictor = DifferentiablePredictor(\n            feature_dim * 2, n_assets, keys[1]\n        )\n        self.decision_maker = DifferentiableDecisionMaker()\n        self.simulator = DifferentiableSimulator()\n    \n    def __call__(\n        self,\n        market_state: MarketState,\n        key: jax.random.PRNGKey\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n        \"\"\"\n        Forward pass through entire pipeline.\n        \n        Returns:\n            portfolio_return: Scalar return\n            intermediates: Dict of intermediate values for analysis\n        \"\"\"\n        # Extract features\n        features = self.feature_extractor(market_state)\n        \n        # Predict\n        expected_returns, predicted_risks = self.predictor(features)\n        \n        # Decide\n        weights = self.decision_maker(expected_returns, predicted_risks)\n        \n        # Simulate\n        portfolio_return = self.simulator(weights, market_state, key)\n        \n        # Store intermediates for gradient analysis\n        intermediates = {\n            'features': features,\n            'expected_returns': expected_returns,\n            'predicted_risks': predicted_risks,\n            'weights': weights,\n            'portfolio_return': portfolio_return\n        }\n        \n        return portfolio_return, intermediates\n\n\ndef smooth_sharpe_loss(returns: jnp.ndarray, epsilon: float = 1e-6) -> jnp.ndarray:\n    \"\"\"Differentiable Sharpe ratio loss.\"\"\"\n    mean_return = jnp.mean(returns)\n    std_return = jnp.sqrt(jnp.var(returns) + epsilon)\n    sharpe = mean_return / std_return\n    return -sharpe  # Negative for minimization\n\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Configuration for E2E-DP training.\"\"\"\n    n_steps: int = 1000\n    batch_size: int = 32\n    learning_rate: float = 1e-3\n    gradient_clip: float = 10.0\n    n_assets: int = 10\n    feature_dim: int = 64\n    seed: int = 42\n\n\ndef generate_synthetic_market_data(\n    n_samples: int,\n    n_assets: int,\n    key: jax.random.PRNGKey\n) -> MarketState:\n    \"\"\"Generate synthetic market data for testing.\"\"\"\n    keys = jax.random.split(key, 4)\n    \n    # Generate correlated price movements\n    price_base = 100 * jnp.ones(n_assets)\n    price_noise = jax.random.normal(keys[0], (n_samples, n_assets))\n    prices = price_base + jnp.cumsum(price_noise * 0.01, axis=0)\n    \n    # Volumes (log-normal)\n    volumes = jnp.exp(jax.random.normal(keys[1], (n_samples, n_assets)) + 10)\n    \n    # Volatilities\n    volatilities = 0.01 + 0.02 * jax.random.uniform(keys[2], (n_samples, n_assets))\n    \n    # Time features (sin/cos encoding)\n    time_idx = jnp.arange(n_samples)\n    time_features = jnp.stack([\n        jnp.sin(2 * jnp.pi * time_idx / 24),  # Daily\n        jnp.cos(2 * jnp.pi * time_idx / 24),\n        jnp.sin(2 * jnp.pi * time_idx / (24 * 7)),  # Weekly\n        jnp.cos(2 * jnp.pi * time_idx / (24 * 7))\n    ], axis=1)\n    \n    # Return last sample as current state\n    return MarketState(\n        prices=prices[-1],\n        volumes=volumes[-1],\n        volatilities=volatilities[-1],\n        time_features=time_features[-1]\n    )\n\n\ndef train_e2e_pipeline(config: TrainingConfig) -> Tuple[EndToEndDPPipeline, Dict[str, Any]]:\n    \"\"\"Train the E2E-DP pipeline with gradient monitoring.\"\"\"\n    \n    key = jax.random.PRNGKey(config.seed)\n    \n    # Initialize pipeline\n    pipeline = EndToEndDPPipeline(\n        n_assets=config.n_assets,\n        feature_dim=config.feature_dim,\n        key=key\n    )\n    \n    # Optimizer\n    optimizer = optax.adam(config.learning_rate)\n    opt_state = optimizer.init(eqx.filter(pipeline, eqx.is_array))\n    \n    # Gradient monitor\n    monitor = EnhancedGradientMonitor()\n    \n    # Training metrics\n    losses = []\n    gradient_healths = []\n    \n    print(\"Training E2E-DP pipeline...\")\n    \n    for step in range(config.n_steps):\n        key, subkey = jax.random.split(key)\n        \n        # Generate batch of market states and simulation keys (stored for reuse)\n        batch_market_states = []\n        batch_sim_keys = []\n        \n        for i in range(config.batch_size):\n            key, data_key, sim_key = jax.random.split(key, 3)\n            \n            # Generate market state\n            market_state = generate_synthetic_market_data(100, config.n_assets, data_key)\n            batch_market_states.append(market_state)\n            batch_sim_keys.append(sim_key)\n        \n        def loss_fn(pipe):\n            # Use the same batch data for gradient computation\n            returns = []\n            for market_state, sim_key in zip(batch_market_states, batch_sim_keys):\n                ret, _ = pipe(market_state, sim_key)\n                returns.append(ret)\n            \n            returns = jnp.array(returns)\n            return smooth_sharpe_loss(returns)\n        \n        # Compute gradients\n        loss, grads = eqx.filter_value_and_grad(loss_fn)(pipeline)\n        \n        # Clip gradients\n        clipped_grads, clipped = apply_global_gradient_clip(grads, config.gradient_clip)\n        \n        # Update\n        updates, opt_state = optimizer.update(clipped_grads, opt_state)\n        pipeline = eqx.apply_updates(pipeline, updates)\n        \n        losses.append(float(loss))\n        \n        # Monitor gradients\n        if step % 10 == 0:\n            metrics = monitor.compute_metrics(grads)\n            is_healthy, issues = metrics.is_healthy()\n            gradient_healths.append(is_healthy)\n            \n            if step % 100 == 0:\n                print(f\"Step {step}: Loss = {loss:.4f}\")\n                print(f\"  Gradient health: {'' if is_healthy else ''}\")\n                if not is_healthy:\n                    print(f\"  Issues: {', '.join(issues)}\")\n                print(f\"  Norm ratio: {metrics.norm_ratio:.2f}\")\n                print(f\"  Clipped: {'Yes' if clipped else 'No'}\")\n    \n    # Final summary\n    health_rate = jnp.mean(jnp.array(gradient_healths)) * 100\n    print(f\"\\nTraining completed!\")\n    print(f\"Final loss: {losses[-1]:.4f}\")\n    print(f\"Gradient health rate: {health_rate:.1f}%\")\n    \n    results = {\n        'losses': jnp.array(losses),\n        'gradient_monitor': monitor,\n        'health_rate': health_rate\n    }\n    \n    return pipeline, results\n\n\nif __name__ == \"__main__\":\n    # Run basic E2E-DP pipeline training\n    config = TrainingConfig(\n        n_steps=500,\n        batch_size=16,\n        learning_rate=1e-3,\n        n_assets=5\n    )\n    \n    pipeline, results = train_e2e_pipeline(config)\n    \n    # Test the trained pipeline\n    print(\"\\nTesting trained pipeline...\")\n    test_key = jax.random.PRNGKey(999)\n    test_market = generate_synthetic_market_data(100, config.n_assets, test_key)\n    \n    test_return, intermediates = pipeline(test_market, test_key)\n    \n    print(f\"Test portfolio return: {test_return:.4f}\")\n    print(f\"Portfolio weights: {intermediates['weights']}\")\n    print(f\"Expected returns: {intermediates['expected_returns']}\")\n    print(f\"Predicted risks: {intermediates['predicted_risks']}\")",
  "crypto_dp/models/portfolio.py": "\"\"\"\nDifferentiable portfolio optimization for crypto trading.\n\nThis module implements end-to-end differentiable portfolio construction\nand optimization using JAX. Supports various risk measures, transaction\ncosts, and portfolio constraints.\n\"\"\"\n\nfrom typing import Tuple, Optional, Callable\nimport logging\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\nfrom jax import jit, grad, vmap\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef softmax_weights(scores: jnp.ndarray, temperature: float = 10.0) -> jnp.ndarray:\n    \"\"\"\n    Convert raw scores to portfolio weights using temperature-scaled softmax.\n    \n    Args:\n        scores: Raw asset scores [n_assets]\n        temperature: Temperature parameter (higher = more uniform)\n    \n    Returns:\n        Portfolio weights that sum to 1\n    \"\"\"\n    scaled_scores = scores / temperature\n    # Numerical stability\n    scaled_scores = scaled_scores - jnp.max(scaled_scores)\n    weights = jnp.exp(scaled_scores)\n    return weights / jnp.sum(weights)\n\n\ndef gumbel_softmax_weights(\n    scores: jnp.ndarray,\n    temperature: float = 1.0,\n    key: Optional[jax.random.PRNGKey] = None\n) -> jnp.ndarray:\n    \"\"\"\n    Gumbel-softmax for differentiable discrete portfolio selection.\n    \n    Args:\n        scores: Raw asset scores [n_assets]\n        temperature: Temperature parameter\n        key: Random key for Gumbel noise\n    \n    Returns:\n        Differentiable discrete-like weights\n    \"\"\"\n    if key is None:\n        key = jax.random.PRNGKey(42)\n    \n    # Sample Gumbel noise\n    gumbel_noise = -jnp.log(-jnp.log(jax.random.uniform(key, scores.shape)))\n    \n    # Gumbel-softmax\n    y = scores + gumbel_noise\n    return softmax_weights(y, temperature)\n\n\ndef long_only_weights(scores: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"\n    Convert scores to long-only portfolio weights.\n    \n    Args:\n        scores: Raw asset scores [n_assets]\n    \n    Returns:\n        Long-only weights (all non-negative, sum to 1)\n    \"\"\"\n    positive_scores = jnp.maximum(scores, 0.0)\n    total = jnp.sum(positive_scores)\n    \n    # Handle edge case where all scores are negative\n    return jnp.where(\n        total > 1e-8,\n        positive_scores / total,\n        jnp.ones_like(scores) / len(scores)  # Equal weights fallback\n    )\n\n\ndef long_short_weights(\n    scores: jnp.ndarray,\n    long_weight: float = 1.0,\n    short_weight: float = 1.0\n) -> jnp.ndarray:\n    \"\"\"\n    Convert scores to long-short portfolio weights.\n    \n    Args:\n        scores: Raw asset scores [n_assets]\n        long_weight: Total weight for long positions\n        short_weight: Total weight for short positions\n    \n    Returns:\n        Long-short weights\n    \"\"\"\n    long_scores = jnp.maximum(scores, 0.0)\n    short_scores = jnp.maximum(-scores, 0.0)\n    \n    # Normalize separately\n    long_sum = jnp.sum(long_scores)\n    short_sum = jnp.sum(short_scores)\n    \n    long_weights = jnp.where(\n        long_sum > 1e-8,\n        long_scores / long_sum * long_weight,\n        0.0\n    )\n    \n    short_weights = jnp.where(\n        short_sum > 1e-8,\n        -short_scores / short_sum * short_weight,\n        0.0\n    )\n    \n    return long_weights + short_weights\n\n\ndef sharpe_ratio(returns: jnp.ndarray, weights: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"\n    Compute differentiable Sharpe ratio.\n    \n    Args:\n        returns: Asset returns [n_periods, n_assets]\n        weights: Portfolio weights [n_assets]\n    \n    Returns:\n        Negative Sharpe ratio (for minimization)\n    \"\"\"\n    portfolio_returns = jnp.dot(returns, weights)\n    mean_return = jnp.mean(portfolio_returns)\n    vol = jnp.std(portfolio_returns)\n    \n    # Add small epsilon for numerical stability\n    sharpe = mean_return / (vol + 1e-8)\n    return -sharpe  # Negative for minimization\n\n\ndef information_ratio(\n    returns: jnp.ndarray,\n    weights: jnp.ndarray,\n    benchmark_returns: jnp.ndarray\n) -> jnp.ndarray:\n    \"\"\"\n    Compute differentiable information ratio.\n    \n    Args:\n        returns: Asset returns [n_periods, n_assets]\n        weights: Portfolio weights [n_assets]\n        benchmark_returns: Benchmark returns [n_periods]\n    \n    Returns:\n        Negative information ratio (for minimization)\n    \"\"\"\n    portfolio_returns = jnp.dot(returns, weights)\n    excess_returns = portfolio_returns - benchmark_returns\n    \n    mean_excess = jnp.mean(excess_returns)\n    tracking_error = jnp.std(excess_returns)\n    \n    ir = mean_excess / (tracking_error + 1e-8)\n    return -ir  # Negative for minimization\n\n\ndef max_drawdown(returns: jnp.ndarray, weights: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"\n    Compute maximum drawdown (approximated for differentiability).\n    \n    Args:\n        returns: Asset returns [n_periods, n_assets]\n        weights: Portfolio weights [n_assets]\n    \n    Returns:\n        Maximum drawdown\n    \"\"\"\n    portfolio_returns = jnp.dot(returns, weights)\n    \n    # Cumulative returns\n    cum_returns = jnp.cumprod(1 + portfolio_returns)\n    \n    # Running maximum (approximated with soft maximum)\n    running_max = jnp.maximum.accumulate(cum_returns)\n    \n    # Drawdowns\n    drawdowns = (cum_returns - running_max) / running_max\n    \n    return jnp.min(drawdowns)  # Most negative drawdown\n\n\ndef transaction_cost_penalty(\n    old_weights: jnp.ndarray,\n    new_weights: jnp.ndarray,\n    cost_rate: float = 0.001\n) -> jnp.ndarray:\n    \"\"\"\n    Compute transaction cost penalty.\n    \n    Args:\n        old_weights: Previous portfolio weights [n_assets]\n        new_weights: New portfolio weights [n_assets]\n        cost_rate: Transaction cost rate (e.g., 0.001 = 0.1%)\n    \n    Returns:\n        Transaction cost penalty\n    \"\"\"\n    turnover = jnp.sum(jnp.abs(new_weights - old_weights))\n    return cost_rate * turnover\n\n\ndef concentration_penalty(weights: jnp.ndarray, max_weight: float = 0.2) -> jnp.ndarray:\n    \"\"\"\n    Penalty for overly concentrated portfolios.\n    \n    Args:\n        weights: Portfolio weights [n_assets]\n        max_weight: Maximum allowed weight per asset\n    \n    Returns:\n        Concentration penalty\n    \"\"\"\n    excess_weights = jnp.maximum(jnp.abs(weights) - max_weight, 0.0)\n    return jnp.sum(excess_weights ** 2)\n\n\nclass DifferentiablePortfolio(eqx.Module):\n    \"\"\"\n    End-to-end differentiable portfolio optimization model.\n    \"\"\"\n    \n    scoring_network: eqx.nn.MLP\n    weight_transform: Callable\n    \n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dims: Tuple[int, ...] = (64, 32),\n        n_assets: int = 10,\n        weight_transform: str = \"softmax\",\n        key: Optional[jax.random.PRNGKey] = None\n    ):\n        \"\"\"\n        Initialize differentiable portfolio model.\n        \n        Args:\n            input_dim: Input feature dimension\n            hidden_dims: Hidden layer dimensions\n            n_assets: Number of assets\n            weight_transform: Weight transformation method\n            key: Random key for initialization\n        \"\"\"\n        if key is None:\n            key = jax.random.PRNGKey(42)\n        \n        # Scoring network\n        self.scoring_network = eqx.nn.MLP(\n            in_size=input_dim,\n            out_size=n_assets,\n            width_size=hidden_dims[0],\n            depth=len(hidden_dims),\n            key=key\n        )\n        \n        # Weight transformation function\n        if weight_transform == \"softmax\":\n            self.weight_transform = softmax_weights\n        elif weight_transform == \"long_only\":\n            self.weight_transform = long_only_weights\n        elif weight_transform == \"long_short\":\n            self.weight_transform = long_short_weights\n        else:\n            raise ValueError(f\"Unknown weight transform: {weight_transform}\")\n    \n    def __call__(self, features: jnp.ndarray) -> jnp.ndarray:\n        \"\"\"\n        Generate portfolio weights from input features.\n        \n        Args:\n            features: Input features [input_dim] or [batch_size, input_dim]\n        \n        Returns:\n            Portfolio weights\n        \"\"\"\n        scores = self.scoring_network(features)\n        \n        if scores.ndim == 1:\n            # Single sample\n            return self.weight_transform(scores)\n        else:\n            # Batch processing\n            return vmap(self.weight_transform)(scores)\n\n\ndef portfolio_objective(\n    model: DifferentiablePortfolio,\n    features: jnp.ndarray,\n    returns: jnp.ndarray,\n    old_weights: Optional[jnp.ndarray] = None,\n    alpha: float = 1.0,\n    beta: float = 0.1,\n    gamma: float = 0.01\n) -> jnp.ndarray:\n    \"\"\"\n    Combined portfolio optimization objective.\n    \n    Args:\n        model: DifferentiablePortfolio model\n        features: Input features for current period\n        returns: Historical returns [n_periods, n_assets]\n        old_weights: Previous portfolio weights (for transaction costs)\n        alpha: Weight for return-based objective (Sharpe ratio)\n        beta: Weight for transaction cost penalty\n        gamma: Weight for concentration penalty\n    \n    Returns:\n        Combined objective (to minimize)\n    \"\"\"\n    # Generate new weights\n    new_weights = model(features)\n    \n    # Sharpe ratio (primary objective)\n    sharpe_loss = sharpe_ratio(returns, new_weights)\n    \n    # Transaction cost penalty\n    if old_weights is not None:\n        tc_penalty = transaction_cost_penalty(old_weights, new_weights)\n    else:\n        tc_penalty = 0.0\n    \n    # Concentration penalty\n    conc_penalty = concentration_penalty(new_weights)\n    \n    return alpha * sharpe_loss + beta * tc_penalty + gamma * conc_penalty\n\n\n@jit\ndef portfolio_step(\n    model: DifferentiablePortfolio,\n    features: jnp.ndarray,\n    returns: jnp.ndarray,\n    old_weights: Optional[jnp.ndarray] = None,\n    learning_rate: float = 1e-3,\n    alpha: float = 1.0,\n    beta: float = 0.1,\n    gamma: float = 0.01\n) -> Tuple[DifferentiablePortfolio, jnp.ndarray, jnp.ndarray]:\n    \"\"\"\n    Single optimization step for portfolio model.\n    \n    Returns:\n        Updated model, loss, and generated weights\n    \"\"\"\n    def objective_fn(model):\n        return portfolio_objective(model, features, returns, old_weights, alpha, beta, gamma)\n    \n    loss, grads = eqx.filter_value_and_grad(objective_fn)(model)\n    updates, _ = optax.sgd(learning_rate).update(grads, None)\n    model = eqx.apply_updates(model, updates)\n    \n    # Generate weights with updated model\n    weights = model(features)\n    \n    return model, loss, weights\n\n\ndef backtest_portfolio(\n    model: DifferentiablePortfolio,\n    features_sequence: jnp.ndarray,\n    returns_sequence: jnp.ndarray,\n    lookback_window: int = 252,\n    rebalance_freq: int = 1\n) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n    \"\"\"\n    Backtest portfolio strategy.\n    \n    Args:\n        model: Trained DifferentiablePortfolio model\n        features_sequence: Feature sequence [n_periods, input_dim]\n        returns_sequence: Return sequence [n_periods, n_assets]\n        lookback_window: Number of periods for return calculation\n        rebalance_freq: Rebalancing frequency (1 = daily)\n    \n    Returns:\n        Portfolio returns, weights over time, transaction costs\n    \"\"\"\n    n_periods = features_sequence.shape[0]\n    n_assets = returns_sequence.shape[1]\n    \n    portfolio_returns = []\n    weights_history = []\n    transaction_costs = []\n    \n    current_weights = jnp.ones(n_assets) / n_assets  # Start with equal weights\n    \n    for t in range(lookback_window, n_periods):\n        if t % rebalance_freq == 0:\n            # Get features for current period\n            features = features_sequence[t]\n            \n            # Historical returns for optimization\n            historical_returns = returns_sequence[t-lookback_window:t]\n            \n            # Generate new weights\n            new_weights = model(features)\n            \n            # Compute transaction costs\n            tc = transaction_cost_penalty(current_weights, new_weights)\n            transaction_costs.append(float(tc))\n            \n            current_weights = new_weights\n        \n        weights_history.append(current_weights)\n        \n        # Compute portfolio return for this period\n        if t < n_periods - 1:  # Avoid index out of bounds\n            period_return = jnp.dot(current_weights, returns_sequence[t])\n            portfolio_returns.append(float(period_return))\n    \n    return (\n        jnp.array(portfolio_returns),\n        jnp.array(weights_history),\n        jnp.array(transaction_costs)\n    )\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    logging.basicConfig(level=logging.INFO)\n    \n    # Create sample data\n    key = jax.random.PRNGKey(42)\n    n_periods, n_assets, input_dim = 1000, 10, 20\n    \n    # Synthetic features and returns\n    features = jax.random.normal(key, (n_periods, input_dim))\n    returns = 0.01 * jax.random.normal(jax.random.split(key)[0], (n_periods, n_assets))\n    \n    # Initialize portfolio model\n    model = DifferentiablePortfolio(\n        input_dim=input_dim,\n        n_assets=n_assets,\n        key=key\n    )\n    \n    # Single optimization step\n    model, loss, weights = portfolio_step(\n        model,\n        features[0],\n        returns[:252],  # Use first 252 periods as history\n        learning_rate=1e-3\n    )\n    \n    logger.info(f\"Optimization step completed. Loss: {loss:.6f}\")\n    logger.info(f\"Generated weights: {weights}\")",
  "crypto_dp/models/__init__.py": "# Differentiable programming models module",
  "crypto_dp/utils/__init__.py": "# Utility functions and helpers"
}